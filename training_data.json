{
  "metadata": {
    "total_samples": 5,
    "distribution": {
      "scientific": 100,
      "government": 0,
      "news": 0,
      "social_media": 0,
      "blog": 0
    }
  },
  "data": [
    {
      "text": "Title: CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning\nAbstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning\nAbstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free\ntrajectories for multiple robots operating in a shared continuous workspace.\nWhile discrete multi-agent path finding (MAPF) methods are broadly adopted due\nto their scalability, their coarse discretization severely limits trajectory\nquality. In contrast, continuous optimization-based planners offer\nhigher-quality paths but suffer from the curse of dimensionality, resulting in\npoor scalability with respect to the number of robots. This paper tackles the\nlimitations of these two approaches by introducing a novel framework that\nintegrates discrete MAPF solvers with constrained generative diffusion models.\nThe resulting framework, called Discrete-Guided Diffusion (DGD), has three key\ncharacteristics: (1) it decomposes the original nonconvex MRMP problem into\ntractable subproblems with convex configuration spaces, (2) it combines\ndiscrete MAPF solutions with constrained optimization techniques to guide\ndiffusion models capture complex spatiotemporal dependencies among robots, and\n(3) it incorporates a lightweight constraint repair mechanism to ensure\ntrajectory feasibility. The proposed method sets a new state-of-the-art\nperformance in large-scale, complex environments, scaling to 100 robots while\nachieving planning efficiency and high success rates.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Multi-channel, multi-template event reconstruction for SuperCDMS data using machine learning\nAbstract: SuperCDMS SNOLAB uses kilogram-scale germanium and silicon detectors to\nsearch for dark matter. Each detector has Transition Edge Sensors (TESs)\npatterned on the top and bottom faces of a large crystal substrate, with the\nTESs electrically grouped into six phonon readout channels per face. Noise\ncorrelations are expected among a detector's readout channels, in part because\nthe channels and their readout electronics are located in close proximity to\none another. Moreover, owing to the large size of the detectors, energy\ndeposits can produce vastly different phonon propagation patterns depending on\ntheir location in the substrate, resulting in a strong position dependence in\nthe readout-channel pulse shapes. Both of these effects can degrade the energy\nresolution and consequently diminish the dark matter search sensitivity of the\nexperiment if not accounted for properly. We present a new algorithm for pulse\nreconstruction, mathematically formulated to take into account correlated noise\nand pulse shape variations. This new algorithm fits N readout channels with a\nsuperposition of M pulse templates simultaneously - hence termed the N$\\times$M\nfilter. We describe a method to derive the pulse templates using principal\ncomponent analysis (PCA) and to extract energy and position information using a\ngradient boosted decision tree (GBDT). We show that these new N$\\times$M and\nGBDT analysis tools can reduce the impact from correlated noise sources while\nimproving the reconstructed energy resolution for simulated mono-energetic\nevents by more than a factor of three and for the 71Ge K-shell electron-capture\npeak recoils measured in a previous version of SuperCDMS called CDMSlite to $<$\n50 eV from the previously published value of $\\sim$100 eV. These results lay\nthe groundwork for position reconstruction in SuperCDMS with the N$\\times$M\noutputs.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Strong Lens Discoveries in DESI Legacy Imaging Surveys DR10 with Two Deep Learning Architectures\nAbstract: We have conducted a search for strong gravitational lensing systems in the\nDark Energy Spectroscopic Instrument (DESI) Legacy Imaging Surveys Data Release\n10 (DR10). This paper is the fourth in a series of searches (following Huang et\nal. 2020; Huang et al. 2021; Storfer et al. 2024, Paper I, II, & III\nrespectively). This is the first catalog of lens candidates covering nearly the\nentirety of the extragalactic sky south of declination $\\delta\\approx +32$ deg,\nall of it observed by the DECam, covering $\\sim$14,000 $deg^2$. We impose a\n$z$-band magnitude cut of < 20 in AB magnitude. We deploy a Residual Neural\nNetwork and EfficientNet as an ensemble trained on a compilation of known\nlensing systems and high-grade candidates as well as nonlenses in the same\nfootprint. The predictions from these two base models are aggregated using a\nmeta-learner. After applying our ensemble to the survey data, we exclude known\ncandidates and systems, and use our own visual inspection portal to rank images\nin the top 0.01 percentile of all neural network recommendations. We have found\n811 new lens candidates. These include 484 new candidates in the Legacy Surveys\nDR9 footprint, all parts of which have been searched for strong lenses at least\nonce before, either by our group or others. Combining the discoveries from this\nwork with those from Paper I (335), II (1210), and III (1512), we have\ndiscovered a total of 3868 new candidates in the DESI Legacy Surveys.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Smart Contract Intent Detection with Pre-trained Programming Language Model\nAbstract: Malicious intent in smart contract development can lead to substantial\neconomic losses. SmartIntentNN is a deep learning model specifically designed\nto identify unsafe intents in smart contracts. This model integrates the\nUniversal Sentence Encoder, a K-means clustering-based intent highlighting\nmechanism, and a Bidirectional Long Short-Term Memory network for multi-label\nclassification, achieving an F1 of 0.8633 in distinguishing ten different\nintent categories. In this study, we present an upgraded version of this model,\nSmartIntentNN2 (Smart Contract Intent Neural Network V2). A significant\nenhancement in V2 is the incorporation of a BERT-based pre-trained language\nmodel, which has been trained on a dataset of 16,000 real smart contracts using\na Masked Language Modeling objective. SmartIntentNN2 retains the BiLSTM-based\nmulti-label classification network. With an improved F1 of 0.927, V2\ndemonstrates enhanced performance compared to its predecessor, establishing\nitself as the state-of-the-art model for smart contract intent detection.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation\nAbstract: Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning\nAbstract: Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication\nAbstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring\nreliable communication remains a formidable challenge, as collapsed\ninfrastructure, unpredictable mobility, and severely constrained resources\ndisrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient\nthrough their store-carry-forward paradigm, reveal the fundamental weaknesses\nof classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when\nconfronted with sparse encounters, buffer shortages, and volatile connectivity.\nTo address these obstacles, this study proposes ML-MaxProp, a hybrid routing\nprotocol that strengthens MaxProp with supervised machine learning. By\nleveraging contextual features such as encounter frequency, hop count, buffer\noccupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay\nsuitability in real time, transforming rigid heuristics into adaptive\nintelligence. Extensive simulations in the ONE environment using the Helsinki\nSPMBM mobility model show that ML-MaxProp consistently surpasses baseline\nprotocols, achieving higher delivery probability, lower latency, and reduced\noverhead. Statistical validation further shows that these improvements are both\nsignificant and robust, even under highly resource-constrained and unstable\nconditions. Overall, this work shows that ML-MaxProp is not just an incremental\nrefinement but a lightweight, adaptive, and practical solution to one of the\nhardest challenges in DTNs: sustaining mission-critical communication when\ninfrastructure collapses and every forwarding decision becomes critical.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Anomaly Detection in Networked Bandits\nAbstract: The nodes' interconnections on a social network often reflect their\ndependencies and information-sharing behaviors. Nevertheless, abnormal nodes,\nwhich significantly deviate from most of the network concerning patterns or\nbehaviors, can lead to grave consequences. Therefore, it is imperative to\ndesign efficient online learning algorithms that robustly learn users'\npreferences while simultaneously detecting anomalies.\n  We introduce a novel bandit algorithm to address this problem. Through\nnetwork knowledge, the method characterizes the users' preferences and\nresiduals of feature information. By learning and analyzing these preferences\nand residuals, it develops a personalized recommendation strategy for each user\nand simultaneously detects anomalies. We rigorously prove an upper bound on the\nregret of the proposed algorithm and experimentally compare it with several\nstate-of-the-art collaborative contextual bandit algorithms on both synthetic\nand real-world datasets.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies\nAbstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: 11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis\nAbstract: For human cognitive process, spatial reasoning and perception are closely\nentangled, yet the nature of this interplay remains underexplored in the\nevaluation of multimodal large language models (MLLMs). While recent MLLM\nadvancements show impressive performance on reasoning, their capacity for\nhuman-like spatial cognition remains an open question. In this work, we\nintroduce a systematic evaluation framework to assess the spatial reasoning\nabilities of state-of-the-art MLLMs relative to human performance. Central to\nour work is 11Plus-Bench, a high-quality benchmark derived from realistic\nstandardized spatial aptitude tests. 11Plus-Bench also features fine-grained\nexpert annotations of both perceptual complexity and reasoning process,\nenabling detailed instance-level analysis of model behavior. Through extensive\nexperiments across 14 MLLMs and human evaluation, we find that current MLLMs\nexhibit early signs of spatial cognition. Despite a large performance gap\ncompared to humans, MLLMs' cognitive profiles resemble those of humans in that\ncognitive effort correlates strongly with reasoning-related complexity.\nHowever, instance-level performance in MLLMs remains largely random, whereas\nhuman correctness is highly predictable and shaped by abstract pattern\ncomplexity. These findings highlight both emerging capabilities and limitations\nin current MLLMs' spatial reasoning capabilities and provide actionable\ninsights for advancing model design.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Neural Conditional Simulation for Complex Spatial Processes\nAbstract: A key objective in spatial statistics is to simulate from the distribution of\na spatial process at a selection of unobserved locations conditional on\nobservations (i.e., a predictive distribution) to enable spatial prediction and\nuncertainty quantification. However, exact conditional simulation from this\npredictive distribution is intractable or inefficient for many spatial process\nmodels. In this paper, we propose neural conditional simulation (NCS), a\ngeneral method for spatial conditional simulation that is based on neural\ndiffusion models. Specifically, using spatial masks, we implement a conditional\nscore-based diffusion model that evolves Gaussian noise into samples from a\npredictive distribution when given a partially observed spatial field and\nspatial process parameters as inputs. The diffusion model relies on a neural\nnetwork that only requires unconditional samples from the spatial process for\ntraining. Once trained, the diffusion model is amortized with respect to the\nobservations in the partially observed field, the number and locations of those\nobservations, and the spatial process parameters, and can therefore be used to\nconditionally simulate from a broad class of predictive distributions without\nretraining the neural network. We assess the NCS-generated simulations against\nsimulations from the true conditional distribution of a Gaussian process model,\nand against Markov chain Monte Carlo (MCMC) simulations from a Brown--Resnick\nprocess model for spatial extremes. In the latter case, we show that it is more\nefficient and accurate to conditionally simulate using NCS than classical MCMC\ntechniques implemented in standard software. We conclude that NCS enables\nefficient and accurate conditional simulation from spatial predictive\ndistributions that are challenging to sample from using traditional methods.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence\nAbstract: Cross-view geo-localization is a critical task for UAV navigation, event\ndetection, and aerial surveying, as it enables matching between drone-captured\nand satellite imagery. Most existing approaches embed multi-modal data into a\njoint feature space to maximize the similarity of paired images. However, these\nmethods typically assume perfect alignment of image pairs during training,\nwhich rarely holds true in real-world scenarios. In practice, factors such as\nurban canyon effects, electromagnetic interference, and adverse weather\nfrequently induce GPS drift, resulting in systematic alignment shifts where\nonly partial correspondences exist between pairs. Despite its prevalence, this\nsource of noisy correspondence has received limited attention in current\nresearch. In this paper, we formally introduce and address the Noisy\nCorrespondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to\nbridge the gap between idealized benchmarks and practical applications. To this\nend, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a\nnovel framework that partitions and augments training data based on estimated\ndata uncertainty through uncertainty-aware co-augmentation and evidential\nco-training. Specifically, PAUL selectively augments regions with high\ncorrespondence confidence and utilizes uncertainty estimation to refine feature\nlearning, effectively suppressing noise from misaligned pairs. Distinct from\ntraditional filtering or label correction, PAUL leverages both data uncertainty\nand loss discrepancy for targeted partitioning and augmentation, thus providing\nrobust supervision for noisy samples. Comprehensive experiments validate the\neffectiveness of individual components in PAUL,which consistently achieves\nsuperior performance over other competitive noisy-correspondence-driven methods\nin various noise ratios.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations\nAbstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its\nexploration through image-based methods remains limited compared to 3D point\ncloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view\nindoor 3D object detector trained without human annotations. In particular,\nOpenM3D is a single-stage detector adapting the 2D-induced voxel features from\nthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic\n3D localization loss requiring high-quality 3D pseudo boxes and a\nvoxel-semantic alignment loss requiring diverse pre-trained CLIP features. We\nfollow the training setting of OV-3DET where posed RGB-D images are given but\nno human annotations of 3D boxes or classes are available. We propose a 3D\nPseudo Box Generation method using a graph embedding technique that combines 2D\nsegments into coherent 3D structures. Our pseudo-boxes achieve higher precision\nand recall than other methods, including the method proposed in OV-3DET. We\nfurther sample diverse CLIP features from 2D segments associated with each\ncoherent 3D structure to align with the corresponding voxel feature. The key to\ntraining a highly accurate single-stage detector requires both losses to be\nlearned toward high-quality targets. At inference, OpenM3D, a highly efficient\ndetector, requires only multi-view images for input and demonstrates superior\naccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor\nbenchmarks compared to existing methods. We outperform a strong two-stage\nmethod that leverages our class-agnostic detector with a ViT CLIP-based OV\nclassifier and a baseline incorporating multi-view depth estimator on both\naccuracy and speed.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Tropical linear series and matroids\nAbstract: We study a notion of tropical linear series on metric graphs that combines\ntwo essential properties of tropicalizations of linear series on algebraic\ncurves: the Baker-Norine rank and the independence rank. Our main results\nrelate the local and global geometry of these tropical linear series to the\ncombinatorial geometry of matroids and valuated matroids, respectively. As an\napplication, we characterize exactly when the tropicalization of the canonical\nlinear series on a single curve is equal to the locus of realizable tropical\ncanonical divisors determined by M\\\"oller, Ulirsch, and Werner. We also\nillustrate our results with a wealth of examples; in particular, we show that\nthe Bergman fan of every matroid appears as the local fan of a tropical linear\nseries on a metric graph. The paper concludes with a list of ten open questions\nfor future investigation.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation\nAbstract: Multimodal semantic communication has great potential to enhance downstream\ntask performance by integrating complementary information across modalities.\nThis paper introduces ProMSC-MIS, a novel Prompt-based Multimodal Semantic\nCommunication framework for Multi-Spectral Image Segmentation. It enables\nefficient task-oriented transmission of spatially aligned RGB and thermal\nimages over band-limited channels. Our framework has two main design novelties.\nFirst, by leveraging prompt learning and contrastive learning, unimodal\nsemantic encoders are pre-trained to learn diverse and complementary semantic\nrepresentations by using features from one modality as prompts for another.\nSecond, a semantic fusion module that combines cross-attention mechanism and\nsqueeze-and-excitation (SE) networks is designed to effectively fuse\ncross-modal features. Experimental results demonstrate that ProMSC-MIS\nsubstantially outperforms conventional image transmission combined with\nstate-of-the-art segmentation methods. Notably, it reduces the required channel\nbandwidth by 50%--70% at the same segmentation performance, while also\ndecreasing the storage overhead and computational complexity by 26% and 37%,\nrespectively. Ablation studies also validate the effectiveness of the proposed\npre-training and semantic fusion strategies. Our scheme is highly suitable for\napplications such as autonomous driving and nighttime surveillance.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks\nAbstract: Failure-Directed Search (FDS) is a significant complete generic search\nalgorithm used in Constraint Programming (CP) to efficiently explore the search\nspace, proven particularly effective on scheduling problems. This paper\nanalyzes FDS's properties, showing that minimizing the size of its search tree\nguided by ranked branching decisions is closely related to the Multi-armed\nbandit (MAB) problem. Building on this insight, MAB reinforcement learning\nalgorithms are applied to FDS, extended with problem-specific refinements and\nparameter tuning, and evaluated on the two most fundamental scheduling\nproblems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained\nProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best\nextended MAB algorithm and configuration, performs 1.7 times faster on the JSSP\nand 2.1 times faster on the RCPSP benchmarks compared to the original\nimplementation in a new solver called OptalCP, while also being 3.5 times\nfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the\ncurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,\nusing only a 900-second time limit per instance, the enhanced FDS improved the\nexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP\nstandard open benchmark instances while also completely closing a few of them.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Correlated decoherence and thermometry with mobile impurities in a 1D Fermi gas\nAbstract: We theoretically investigate the correlated decoherence dynamics of two\nmobile impurities trapped within a gas of ultracold fermionic atoms. We use a\nmean-field approximation to self-consistently describe the effect of\nimpurity-gas collisions on impurity motion, while decoherence of the\nimpurities' internal state is computed exactly within a functional determinant\napproach. At equilibrium, we find that the impurities undergo bath-induced\nlocalisation as the impurity-gas interaction strength is increased. We then\nstudy the non-equilibrium dynamics induced by a sudden change of the\nimpurities' internal state, which can be experimentally probed by Ramsey\ninterferometry. Our theoretical approach allows us to investigate the effect of\nimpurity motion on decoherence dynamics, finding strong deviations from the\nuniversal behaviour associated with Anderson's orthogonality catastrophe when\nthe mass imbalance between impurity and gas atoms is small. Finally, we show\nthat mobile impurities can be used as thermometers of their environment and\nthat bath-mediated correlations can be beneficial for thermometric performance\nat low temperatures, even in the presence of non-trivial impurity motion. Our\nresults showcase the interesting open quantum dynamics of mobile impurities\ndephasing in a common environment, and could help provide more precise\ntemperature estimates of ultracold fermionic mixtures.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Temperature induced optical scatter changes in titania-germania coatings\nAbstract: Titania doped with tantala is the high index material (high n) for the\noptical coatings used in LIGO and Virgo and its thermal noise limits LIGO/Virgo\nobservations of astrophysical sources. In this paper, we study temperature\ninduced changes to optical scatter of a multilayer highly reflective coating\ncomprised of silica (low n) and titania doped with germania (high n) as a\npotential candidate to reduce coating thermal noise in ground-based\nobservatories operating at room temperature. We observe that the scatter\nmeasured at 8 degree in a small region is low, with a median starting BRDF of\n$1.1 \\times 10^{-7}\\,\\mathrm{str}^{-1}$ increasing to $1.2 \\times\n10^{-6}\\,\\mathrm{str}^{-1}$ through annealing. The results presented here show\nthe potential of adopting titania doped with germania coatings for future\nupgrades to LIGO and Virgo and as a pathfinder coating for Cosmic Explorer, a\nnext-generation detector.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Model Science: getting serious about verification, explanation and control of AI systems\nAbstract: The growing adoption of foundation models calls for a paradigm shift from\nData Science to Model Science. Unlike data-centric approaches, Model Science\nplaces the trained model at the core of analysis, aiming to interact, verify,\nexplain, and control its behavior across diverse operational contexts. This\npaper introduces a conceptual framework for a new discipline called Model\nScience, along with the proposal for its four key pillars: Verification, which\nrequires strict, context-aware evaluation protocols; Explanation, which is\nunderstood as various approaches to explore of internal model operations;\nControl, which integrates alignment techniques to steer model behavior; and\nInterface, which develops interactive and visual explanation tools to improve\nhuman calibration and decision-making. The proposed framework aims to guide the\ndevelopment of credible, safe, and human-aligned AI systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Robust Paths: Geometry and Computation\nAbstract: Applying robust optimization often requires selecting an appropriate\nuncertainty set both in shape and size, a choice that directly affects the\ntrade-off between average-case and worst-case performances. In practice, this\ncalibration is usually done via trial-and-error: solving the robust\noptimization problem many times with different uncertainty set shapes and\nsizes, and examining their performance trade-off. This process is\ncomputationally expensive and ad hoc. In this work, we take a principled\napproach to study this issue for robust optimization problems with linear\nobjective functions, convex feasible regions, and convex uncertainty sets. We\nintroduce and study what we define as the robust path: a set of robust\nsolutions obtained by varying the uncertainty set's parameters. Our central\ngeometric insight is that a robust path can be characterized as a Bregman\nprojection of a curve (whose geometry is defined by the uncertainty set) onto\nthe feasible region. This leads to a surprising discovery that the robust path\ncan be approximated via the trajectories of standard optimization algorithms,\nsuch as the proximal point method, of the deterministic counterpart problem. We\ngive a sharp approximation error bound and show it depends on the geometry of\nthe feasible region and the uncertainty set. We also illustrate two special\ncases where the approximation error is zero: the feasible region is\npolyhedrally monotone (e.g., a simplex feasible region under an ellipsoidal\nuncertainty set), or the feasible region and the uncertainty set follow a dual\nrelationship. We demonstrate the practical impact of this approach in two\nsettings: portfolio optimization and adversarial deep learning.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Eigenvalue distribution of the Neural Tangent Kernel in the quadratic scaling\nAbstract: We compute the asymptotic eigenvalue distribution of the neural tangent\nkernel of a two-layer neural network under a specific scaling of dimension.\nNamely, if $X\\in\\mathbb{R}^{n\\times d}$ is an i.i.d random matrix,\n$W\\in\\mathbb{R}^{d\\times p}$ is an i.i.d $\\mathcal{N}(0,1)$ matrix and\n$D\\in\\mathbb{R}^{p\\times p}$ is a diagonal matrix with i.i.d bounded entries,\nwe consider the matrix\n  \\[\n  \\mathrm{NTK}\n  =\n  \\frac{1}{d}XX^\\top\n  \\odot\n  \\frac{1}{p}\n  \\sigma'\\left(\n  \\frac{1}{\\sqrt{d}}XW\n  \\right)D^2\n  \\sigma'\\left(\n  \\frac{1}{\\sqrt{d}}XW\n  \\right)^\\top\n  \\]\n  where $\\sigma'$ is a pseudo-Lipschitz function applied entrywise and under\nthe scaling $\\frac{n}{dp}\\to \\gamma_1$ and $\\frac{p}{d}\\to \\gamma_2$. We\ndescribe the asymptotic distribution as the free multiplicative convolution of\nthe Marchenko--Pastur distribution with a deterministic distribution depending\non $\\sigma$ and $D$.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Pruning Strategies for Backdoor Defense in LLMs\nAbstract: Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Large Language Models (LLMs) for Electronic Design Automation (EDA)\nAbstract: With the growing complexity of modern integrated circuits, hardware engineers\nare required to devote more effort to the full design-to-manufacturing\nworkflow. This workflow involves numerous iterations, making it both\nlabor-intensive and error-prone. Therefore, there is an urgent demand for more\nefficient Electronic Design Automation (EDA) solutions to accelerate hardware\ndevelopment. Recently, large language models (LLMs) have shown remarkable\nadvancements in contextual comprehension, logical reasoning, and generative\ncapabilities. Since hardware designs and intermediate scripts can be\nrepresented as text, integrating LLM for EDA offers a promising opportunity to\nsimplify and even automate the entire workflow. Accordingly, this paper\nprovides a comprehensive overview of incorporating LLMs into EDA, with emphasis\non their capabilities, limitations, and future opportunities. Three case\nstudies, along with their outlook, are introduced to demonstrate the\ncapabilities of LLMs in hardware design, testing, and optimization. Finally,\nfuture directions and challenges are highlighted to further explore the\npotential of LLMs in shaping the next-generation EDA, providing valuable\ninsights for researchers interested in leveraging advanced AI technologies for\nEDA.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Using item recommendations and LLMs in marketing email titles\nAbstract: E-commerce marketplaces make use of a number of marketing channels like\nemails, push notifications, etc. to reach their users and stimulate purchases.\nPersonalized emails especially are a popular touch point for marketers to\ninform users of latest items in stock, especially for those who stopped\nvisiting the marketplace. Such emails contain personalized recommendations\ntailored to each user's interests, enticing users to buy relevant items. A\ncommon limitation of these emails is that the primary entry point, the title of\nthe email, tends to follow fixed templates, failing to inspire enough interest\nin the contents. In this work, we explore the potential of large language\nmodels (LLMs) for generating thematic titles that reflect the personalized\ncontent of the emails. We perform offline simulations and conduct online\nexperiments on the order of millions of users, finding our techniques useful in\nimproving the engagement between customers and our emails. We highlight key\nfindings and learnings as we productionize the safe and automated generation of\nemail titles for millions of users.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring\nAbstract: Sensitive attributes like gender or age can lead to unfair predictions in\nmachine learning tasks such as predictive business process monitoring,\nparticularly when used without considering context. We present FairLoop1, a\ntool for human-guided bias mitigation in neural network-based prediction\nmodels. FairLoop distills decision trees from neural networks, allowing users\nto inspect and modify unfair decision logic, which is then used to fine-tune\nthe original model towards fairer predictions. Compared to other approaches to\nfairness, FairLoop enables context-aware bias removal through human\ninvolvement, addressing the influence of sensitive attributes selectively\nrather than excluding them uniformly.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence\nAbstract: Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control\nAbstract: The rapid advancement of large vision language models (LVLMs) and agent\nsystems has heightened interest in mobile GUI agents that can reliably\ntranslate natural language into interface operations. Existing single-agent\napproaches, however, remain limited by structural constraints. Although\nmulti-agent systems naturally decouple different competencies, recent progress\nin multi-agent reinforcement learning (MARL) has often been hindered by\ninefficiency and remains incompatible with current LVLM architectures. To\naddress these challenges, we introduce SWIRL, a staged workflow for interleaved\nreinforcement learning designed for multi-agent systems. SWIRL reformulates\nMARL into a sequence of single-agent reinforcement learning tasks, updating one\nagent at a time while keeping the others fixed. This formulation enables stable\ntraining and promotes efficient coordination across agents. Theoretically, we\nprovide a stepwise safety bound, a cross-round monotonic improvement theorem,\nand convergence guarantees on return, ensuring robust and principled\noptimization. In application to mobile GUI control, SWIRL instantiates a\nNavigator that converts language and screen context into structured plans, and\nan Interactor that grounds these plans into executable atomic actions.\nExtensive experiments demonstrate superior performance on both high-level and\nlow-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong\ncapability in multi-agent mathematical reasoning, underscoring its potential as\na general framework for developing efficient and robust multi-agent systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling\nAbstract: Schedulers are critical for optimal resource utilization in high-performance\ncomputing. Traditional methods to evaluate schedulers are limited to\npost-deployment analysis, or simulators, which do not model associated\ninfrastructure. In this work, we present the first-of-its-kind integration of\nscheduling and digital twins in HPC. This enables what-if studies to understand\nthe impact of parameter configurations and scheduling decisions on the physical\nassets, even before deployment, or regarching changes not easily realizable in\nproduction. We (1) provide the first digital twin framework extended with\nscheduling capabilities, (2) integrate various top-tier HPC systems given their\npublicly available datasets, (3) implement extensions to integrate external\nscheduling simulators. Finally, we show how to (4) implement and evaluate\nincentive structures, as-well-as (5) evaluate machine learning based\nscheduling, in such novel digital-twin based meta-framework to prototype\nscheduling. Our work enables what-if scenarios of HPC systems to evaluate\nsustainability, and the impact on the simulated system.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment\nAbstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach\nAbstract: This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Bipartite Matching with Pair-Dependent Bounds\nAbstract: Let $G=(U \\cup V, E)$ be a bipartite graph, where $U$ represents jobs and $V$\nrepresents machines. We study a new variant of the bipartite matching problem\nin which each job in $U$ can be matched to at most one machine in $V$, and the\nnumber of jobs that can be assigned to a machine depends on the specific jobs\nmatched to it. These pair-dependent bounds reflect systems where different jobs\nhave varying tolerance for congestion, determined by the specific machine they\nare assigned to.\n  We define a bipartite PD-matching as a set of edges $M \\subseteq E$ that\nsatisfies these job-to-machine tolerance constraints. This variant of matching\nextends well-known matching problems, however, despite its relevance to\nreal-world systems, it has not been studied before. We study bipartite\nPD-matchings with the objective of maximizing the matching size. As we show,\nthe problem exhibits significant differences from previously studied matching\nproblems. We analyze its computational complexity both in the general case and\nfor specific restricted instances, presenting hardness results alongside\noptimal and approximation algorithms.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation\nAbstract: This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Selective Retrieval-Augmentation for Long-Tail Legal Text Classification\nAbstract: Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification. The code repository is available at:\nhttps://github.com/Boheng-Mao/sra-legal",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning\nAbstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but\noften suffers from degraded performance when exposed to low-quality data.\nSupervision errors in early turns can propagate across subsequent turns,\nundermining coherence and response quality. Existing methods typically address\ndata quality via static prefiltering, which decouples quality control from\ntraining and fails to mitigate turn-level error propagation. In this context,\nwe propose ReSURE (Regularizing Supervision UnREliability), an adaptive\nlearning method that dynamically down-weights unreliable supervision without\nexplicit filtering. ReSURE estimates per-turn loss distributions using\nWelford's online statistics and reweights sample losses on the fly accordingly.\nExperiments on both single-source and mixed-quality datasets show improved\nstability and response quality. Notably, ReSURE enjoys positive Spearman\ncorrelations (0.21 ~ 1.0 across multiple benchmarks) between response scores\nand number of samples regardless of data quality, which potentially paves the\nway for utilizing large-scale data effectively. Code is publicly available at\nhttps://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: MathBuddy: A Multimodal System for Affective Math Tutoring\nAbstract: The rapid adoption of LLM-based conversational systems is already\ntransforming the landscape of educational technology. However, the current\nstate-of-the-art learning models do not take into account the student's\naffective states. Multiple studies in educational psychology support the claim\nthat positive or negative emotional states can impact a student's learning\ncapabilities. To bridge this gap, we present MathBuddy, an emotionally aware\nLLM-powered Math Tutor, which dynamically models the student's emotions and\nmaps them to relevant pedagogical strategies, making the tutor-student\nconversation a more empathetic one. The student's emotions are captured from\nthe conversational text as well as from their facial expressions. The student's\nemotions are aggregated from both modalities to confidently prompt our LLM\nTutor for an emotionally-aware response. We have effectively evaluated our\nmodel using automatic evaluation metrics across eight pedagogical dimensions\nand user studies. We report a massive 23 point performance gain using the win\nrate and a 3 point gain at an overall level using DAMR scores which strongly\nsupports our hypothesis of improving LLM-based tutor's pedagogical abilities by\nmodeling students' emotions.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Self-Supervised Pre-Training with Equilibrium Constraints\nAbstract: Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Waiting around for Unruh\nAbstract: How long does a uniformly rotating observer need to interact with a quantum\nfield in order to register an approximately thermal response due to the\ncircular motion Unruh effect? We address this question for a massless scalar\nfield in 2+1 dimensions, defining the effective temperature via the ratio of\nexcitation and de-excitation rates of an Unruh-DeWitt detector in the long\ninteraction time limit. In this system, the effective temperature is known to\nbe significantly smaller than the linear motion Unruh effect prediction when\nthe detector's energy gap is small: the effective temperature tends to zero in\nthe small gap limit, linearly in the gap. We show that a positive small gap\ntemperature at long interaction times can be regained via a controlled\nlong-time-small-gap double limit, provided the detector's coupling to the field\nis allowed to change sign. The resulting small gap temperature depends on the\nparameters of the circular motion but not on the details of the detector's\nswitching. The results broaden the energy range for pursuing an experimental\nverification of the circular motion Unruh effect in analogue spacetime\nexperiments. As a mathematical tool, we provide a new implementation of the\nlong interaction time limit that controls in a precise way the asymptotics of\nboth the switching function and its Fourier transform.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Evaluating Language Model Reasoning about Confidential Information\nAbstract: As language models are increasingly deployed as autonomous agents in\nhigh-stakes settings, ensuring that they reliably follow user-defined rules has\nbecome a critical safety concern. To this end, we study whether language models\nexhibit contextual robustness, or the capability to adhere to context-dependent\nsafety specifications. For this analysis, we develop a benchmark (PasswordEval)\nthat measures whether language models can correctly determine when a user\nrequest is authorized (i.e., with a correct password). We find that current\nopen- and closed-source models struggle with this seemingly simple task, and\nthat, perhaps surprisingly, reasoning capabilities do not generally improve\nperformance. In fact, we find that reasoning traces frequently leak\nconfidential information, which calls into question whether reasoning traces\nshould be exposed to users in such applications. We also scale the difficulty\nof our evaluation along multiple axes: (i) by adding adversarial user pressure\nthrough various jailbreaking strategies, and (ii) through longer multi-turn\nconversations where password verification is more challenging. Overall, our\nresults suggest that current frontier models are not well-suited to handling\nconfidential information, and that reasoning capabilities may need to be\ntrained in a different manner to make them safer for release in high-stakes\nsettings.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Reducing Street Parking Search Time via Smart Assignment Strategies\nAbstract: In dense metropolitan areas, searching for street parking adds to traffic\ncongestion. Like many other problems, real-time assistants based on mobile\nphones have been proposed, but their effectiveness is understudied. This work\nquantifies how varying levels of user coordination and information availability\nthrough such apps impact search time and the probability of finding street\nparking. Through a data-driven simulation of Madrid's street parking ecosystem,\nwe analyze four distinct strategies: uncoordinated search (Unc-Agn),\ncoordinated parking without awareness of non-users (Cord-Agn), an idealized\noracle system that knows the positions of all non-users (Cord-Oracle), and our\nnovel/practical Cord-Approx strategy that estimates non-users' behavior\nprobabilistically. The Cord-Approx strategy, instead of requiring knowledge of\nhow close non-users are to a certain spot in order to decide whether to\nnavigate toward it, uses past occupancy distributions to elongate physical\ndistances between system users and alternative parking spots, and then solves a\nHungarian matching problem to dispatch accordingly. In high-fidelity\nsimulations of Madrid's parking network with real traffic data, users of\nCord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes\nfor non-users without an app. A zone-level snapshot shows that Cord-Approx\nreduces search time for system users by 72% (range = 67-76%) in central hubs,\nand up to 73% in residential areas, relative to non-users.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning\nAbstract: This study presents a machine learning framework for forecasting short-term\nfaults in industrial centrifugal pumps using real-time sensor data. The\napproach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in\nadvance based on patterns extracted from historical operation. Two lookback\nperiods, 60 minutes and 120 minutes, were evaluated using a sliding window\napproach. For each window, statistical features including mean, standard\ndeviation, minimum, maximum, and linear trend were extracted, and class\nimbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost\nclassifiers were trained and tested on the labeled dataset. Results show that\nthe Random Forest model achieved the best short-term forecasting performance\nwith a 60-minute window, reaching recall scores of 69.2\\% at 5 minutes, 64.9\\%\nat 15 minutes, and 48.6\\% at 30 minutes. With a 120-minute window, the Random\nForest model achieved 57.6\\% recall at 5 minutes, and improved predictive\naccuracy of 65.6\\% at both 15 and 30 minutes. XGBoost displayed similar but\nslightly lower performance. These findings highlight that optimal history\nlength depends on the prediction horizon, and that different fault patterns may\nevolve at different timescales. The proposed method offers an interpretable and\nscalable solution for integrating predictive maintenance into real-time\nindustrial monitoring systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation\nAbstract: Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants\nAbstract: Optimizing modern production plants using the job-shop principle is a known\nhard problem. For very large plants, like semiconductor fabs, the problem\nbecomes unsolvable on a plant-wide scale in a reasonable amount of time using\nclassical linear optimization. An alternative approach is the use of swarm\nintelligence algorithms. These have been applied to the job-shop problem\nbefore, but often in a centrally calculated way where they are applied to the\nsolution space, but they can be implemented in a bottom-up fashion to avoid\nglobal result computation as well. One of the problems in semiconductor\nproduction is that the production process requires a lot of switching between\nmachines that process lots one after the other and machines that process\nbatches of lots at once, often with long processing times. In this paper, we\naddress this switching problem with the ``boids'' flocking algorithm that was\noriginally used in robotics and movie industry. The flocking behavior is a\nbio-inspired algorithm that uses only local information and interaction based\non simple heuristics. We show that this algorithm addresses these valid\nconsiderations in production plant optimization, as it reacts to the switching\nof machine kinds similar to how a swarm of flocking animals would react to\nobstacles in its course.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Constraining Ricci-Cubic Holographic Dark Energy from observational data using the MCMC sampling and enhanced Machine learning analysis\nAbstract: In this work, we find constraints on the parameter space of the Ricci-Cubic\nHolographic dark energy (RCHDE) from various observational data sets like\nHubble data, cosmic-chronometer data, Baryon-acoustic oscillation data, and\nalso data from gamma-ray bursts. RCHDE is formed from the cubic invariant,\nwhich in turn is built from the cubic contractions of the Riemann and Ricci\ntensors. We have used the Markov chain Monte-Carlo (MCMC) sampling technique to\nfind constraints on the model parameters via Bayesian inference. Contour plots\nhave been obtained for the model parameters, showing their marginalized and\njoint probability distributions. The best-fit regression lines are found for\nthe constrained model and compared with the standard $\\Lambda$CDM model to\nverify and validate the model. To complement this data analysis mechanism, we\nhave also performed an enhanced machine learning analysis using observational\nHubble parameter data. This approach serves to validate the model's predictive\npower through independent, data-driven regression techniques. Different\ngraphical illustrations of the machine learning techniques have been presented\nto understand the results. These illustrations reveal a strong agreement\nbetween the Hubble parameter predictions from the machine learning models, the\ntheoretical RCHDE model, and observational data.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Regular and Floquet bases for gauge and gravity theories: a non perturbative approach\nAbstract: The main topic of the paper is represented by the change of basis, in\nHeun-type equations, from the one of decaying (at two singular points)\nsolutions to that of Floquet solutions. Crucial in the connection relations is\nthe phase acquired by the Floquet solutions by going from a (ir)regular\nsingularity to another. The new 'kink method' is exploited to compute the\nquantum momentum of the Floquet solutions as convergent series, explicitly at\nall orders. Hence, upon integrating it term by term, the acquired phase can be\nderived explicitly as a similar series. Since Heun equation and its confluences\ndescribe ${\\cal N}=2$ SYM theories in the NS background as quantisation of\nSeiberg-Witten differentials and also appear in perturbations of gravity\nsolutions, tests and predictions in both domains can be made. A very\nencouraging test is the matching of the acquired phase with the dual gauge\nperiod, $A_D$, as given by the Nekrasov instanton function. Actually, the\nprocedure can be considered an alternative to instanton computations. On the\ngravity side, results for the wave functions are at leading order compatible\nwith analogous expressions found by studying the Teukolsky (or others, like\nRegge-Wheeler) equation. Besides, the whole construction can represent an\nuseful approach to these equations at all orders, shedding also light on\nnon-perturbative contributions, which should reveal very interesting for the\nconsequences in gravity perturbations.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Long-VLA: Unleashing Long-Horizon Capability of Vision Language Action Model for Robot Manipulation\nAbstract: Vision-Language-Action (VLA) models have become a cornerstone in robotic\npolicy learning, leveraging large-scale multimodal data for robust and scalable\ncontrol. However, existing VLA frameworks primarily address short-horizon\ntasks, and their effectiveness on long-horizon, multi-step robotic manipulation\nremains limited due to challenges in skill chaining and subtask dependencies.\nIn this work, we introduce Long-VLA, the first end-to-end VLA model\nspecifically designed for long-horizon robotic tasks. Our approach features a\nnovel phase-aware input masking strategy that adaptively segments each subtask\ninto moving and interaction phases, enabling the model to focus on\nphase-relevant sensory cues and enhancing subtask compatibility. This unified\nstrategy preserves the scalability and data efficiency of VLA training, and our\narchitecture-agnostic module can be seamlessly integrated into existing VLA\nmodels. We further propose the L-CALVIN benchmark to systematically evaluate\nlong-horizon manipulation. Extensive experiments on both simulated and\nreal-world tasks demonstrate that Long-VLA significantly outperforms prior\nstate-of-the-art methods, establishing a new baseline for long-horizon robotic\ncontrol.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Global Permutation Entropy\nAbstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used\ncomplexity measure for real-valued time series that is based on the relative\norder of values within consecutive segments of fixed length. After\nstandardizing each segment to a permutation and computing the frequency\ndistribution of these permutations, Shannon Entropy is then applied to quantify\nthe series' complexity. We introduce Global Permutation Entropy (GPE), a novel\nindex that considers all possible patterns of a given length, including\nnon-consecutive ones. Its computation relies on recently developed algorithms\nthat enable the efficient extraction of full permutation profiles. We\nillustrate some properties of GPE and demonstrate its effectiveness through\nexperiments on synthetic datasets, showing that it reveals structural\ninformation not accessible through standard permutation entropy. We provide a\nJulia package for the calculation of GPE at\n`https://github.com/AThreeH1/Global-Permutation-Entropy'.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and Style Priors\nAbstract: Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Closing the Loop: Integrating Material Needs of Energy Technologies into Energy System Models\nAbstract: The transition to a climate-neutral energy system demands large-scale\nrenewable generation expansion, which requires substantial amounts of bulk\nmaterials like steel, cement, and polymers. The production of these materials\nrepresents an additional energy demand for the system, creating an\nenergy-material feedback loop. Current energy system models lack a complete\nrepresentation of this feedback loop. Material requirements of energy system\ntransformation have been studied in a retrospective approach, not allowing them\nas a consideration in system design. To address this gap, we integrate bulk\nmaterial demand and production as endogenous factors into energy system\noptimization using PyPSA-Eur. Our approach links infrastructure expansion with\nindustrial energy needs to achieve a minimum-cost equilibrium. Applying this\nmodel to Germany's transition to climate neutrality by 2045, we find that\naccounting for material needs increases annual bulk material demands by 3-9 %,\nshifts preferences from solar to wind and from local production of hydrogen to\nship imports, and shows distinct industrial process route choices. These\nfindings suggests that energy-material feedbacks should be considered in energy\nsystem design when moving to more domestic production of energy technologies.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions\nAbstract: We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Laser-induced transient opacity in helium nanodroplets probed by single-shot coherent diffraction\nAbstract: Single-shot coherent diffractive imaging (CDI) with intense short-wavelength\nlight pulses enables the structural characterization of individual\nnanoparticles in free flight with high spatial and temporal resolution.\nConventional CDI assumes that the target object exhibits a linear scattering\nresponse and static electronic properties. Here, we extend this approach to\ninvestigate transient laser-driven modifications of the electronic structure in\nindividual nanoparticles, imprinted in their time-resolved diffraction\npatterns. In the presence of a near-infrared laser pulse, we observe a\npronounced reduction in the diffraction signal from helium nanodroplets when\nprobed with ultrashort extreme ultraviolet (XUV) pulses. This effect is\nattributed to a light-field-induced modification of the electronic structure of\nthe droplets, which substantially increases their XUV absorption. Our results\ndemonstrate that single-particle diffraction can capture ultrafast light-driven\nelectron dynamics in nanoscale systems. This paves the way for the\nspatiotemporal tracking of reversible changes in the electronic properties of\nnanoscale structures with potential applications in ultrafast X-ray optics,\nmaterials science, and all-optical signal processing.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Quantitative stability for the conformally invariant Chang-Gui inequality on the exponentiation of functions on the sphere\nAbstract: In this work, we focus on a recent variant of the Trudinger-Moser-Onofri\ninequality introduced by S. Y. Alice Chang and Changfeng Gui \\cite{CG-2023}:\n\\begin{align*} \\alpha\\int_{\\mathbb{S}^2}|\\nabla_{\\mathbb{S}^2}u|^2 {\\rm\nd}\\omega+2 \\int_{\\mathbb{S}^2} u {\\rm d}\\omega\n-\\frac{1}{2}\\ln\\left[\\left(\\int_{\\mathbb{s}^2}e^{2u}{\\rm\nd}\\omega\\right)^2-\\sum_{i=1}^3\\left(\\int_{\\mathbb{s}^2}\\omega_i e^{2u}{\\rm d}\n\\omega\\right)^2\\right] \\geq 0 \\end{align*} holds on $H^1(\\mathbb{S}^2)$ if and\nonly if $\\alpha \\geq \\frac{2}{3}$. In this regime, the infimum is attained only\nby trivial functions when $\\alpha > \\frac{2}{3},$ whereas for the critical\nvalue $\\alpha = \\frac{2}{3}$ nontrivial extremals exist, and Chang-Gui further\nprovided a complete classification of such solutions.\n  Building upon their result, we found a nice conformal invariance of the\nassociated functional. Exploiting this invariance, we were able to characterize\nthe full family of extremals in terms of conformal maps of $\\mathbb{S}^2$ and,\nmoreover, establish a sharp quantitative stability result in the gradient norm.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: FARM: Frame-Accelerated Augmentation and Residual Mixture-of-Experts for Physics-Based High-Dynamic Humanoid Control\nAbstract: Unified physics-based humanoid controllers are pivotal for robotics and\ncharacter animation, yet models that excel on gentle, everyday motions still\nstumble on explosive actions, hampering real-world deployment. We bridge this\ngap with FARM (Frame-Accelerated Augmentation and Residual Mixture-of-Experts),\nan end-to-end framework composed of frame-accelerated augmentation, a robust\nbase controller, and a residual mixture-of-experts (MoE). Frame-accelerated\naugmentation exposes the model to high-velocity pose changes by widening\ninter-frame gaps. The base controller reliably tracks everyday low-dynamic\nmotions, while the residual MoE adaptively allocates additional network\ncapacity to handle challenging high-dynamic actions, significantly enhancing\ntracking accuracy. In the absence of a public benchmark, we curate the\nHigh-Dynamic Humanoid Motion (HDHM) dataset, comprising 3593 physically\nplausible clips. On HDHM, FARM reduces the tracking failure rate by 42.8\\% and\nlowers global mean per-joint position error by 14.6\\% relative to the baseline,\nwhile preserving near-perfect accuracy on low-dynamic motions. These results\nestablish FARM as a new baseline for high-dynamic humanoid control and\nintroduce the first open benchmark dedicated to this challenge. The code and\ndataset will be released at https://github.com/Colin-Jing/FARM.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification\nAbstract: Network traffic classification using pre-training models has shown promising\nresults, but existing methods struggle to capture packet structural\ncharacteristics, flow-level behaviors, hierarchical protocol semantics, and\ninter-packet contextual relationships. To address these challenges, we propose\nFlowletFormer, a BERT-based pre-training model specifically designed for\nnetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware\nTraffic Representation Model for segmenting traffic into semantically\nmeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture\nmultilayer protocol semantics, and Field-Specific and Context-Aware Pretraining\nTasks to enhance both inter-packet and inter-flow learning. Experimental\nresults demonstrate that FlowletFormer significantly outperforms existing\nmethods in the effectiveness of traffic representation, classification\naccuracy, and few-shot learning capability. Moreover, by effectively\nintegrating domain-specific network knowledge, FlowletFormer shows better\ncomprehension of the principles of network transmission (e.g., stateful\nconnections of TCP), providing a more robust and trustworthy framework for\ntraffic analysis.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: HEAL: A Hypothesis-Based Preference-Aware Analysis Framework\nAbstract: Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Walk the Robot: Exploring Soft Robotic Morphological Communication driven by Spiking Neural Networks\nAbstract: Recently, researchers have explored control methods that embrace nonlinear\ndynamic coupling instead of suppressing it. Such designs leverage dynamical\ncoupling for communication between different parts of the robot. Morphological\ncommunication refers to when those dynamics can be used as an emergent data bus\nto facilitate coordination among independent controller modules within the same\nrobot. Previous research with tensegrity-based robot designs has shown that\nevolutionary learning models that evolve spiking neural networks (SNN) as robot\ncontrol mechanisms are effective for controlling non-rigid robots. Our own\nresearch explores the emergence of morphological communication in an SNN-based\nsimulated soft robot in theEvoGym environment.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: An Approach to Estimating Quadratic Logistic Model Parameters of Fractal Dimension Curves\nAbstract: The fractal dimension curves of urban form and growth fall into two\ncategories: One can be described by common logistic function, and the other can\nbe described with quadratic logistic function. The approach to estimating the\nparameter of the ordinary logistic model has been developed. However, how to\nestimate the parameter of quadratic logistic model is still a problem. This\npaper is devoted to finding a nonlinear regressive approach for estimating\nparameter values of quadratic logistic model of fractal dimension curves. The\nprocess can be summarized as below. First, differentiating quadratic logistic\nfunction in theory with respect to time yields a growth rate equation of\nfractal dimension. Second, discretizing the growth rate equation yields a\nnonlinear regressive model of fractal dimension curve. Third, applying the\nleast squares method to the nonlinear regressive equation yields the capacity\nparameter value of the quadratic logistic model. Fourth, substituting the\ncapacity parameter value into the quadratic logistic model and changing it into\na quasilinear form, we can estimate the other parameter values by ordinary\nlinear regression analysis. In this way, a practical quadratic logistic model\nof fractal dimension curves can be gained. The approach is applied to\nmultifractal dimension curves of Beijing city to show its effectiveness. The\nmethod can be extended to estimate the parameter values of quadratic logistic\nmodels in many fields besides urban science.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling\nAbstract: Retrieval-augmented learning based on radiology reports has emerged as a\npromising direction to improve performance on long-tail medical imaging tasks,\nsuch as rare disease detection in chest X-rays. Most existing methods rely on\ncomparing high-dimensional text embeddings from models like CLIP or CXR-BERT,\nwhich are often difficult to interpret, computationally expensive, and not\nwell-aligned with the structured nature of medical knowledge. We propose a\nnovel, ontology-driven alternative for comparing radiology report texts based\non clinically grounded concepts from the Unified Medical Language System\n(UMLS). Our method extracts standardised medical entities from free-text\nreports using an enhanced pipeline built on RadGraph-XL and SapBERT. These\nentities are linked to UMLS concepts (CUIs), enabling a transparent,\ninterpretable set-based representation of each report. We then define a\ntask-adaptive similarity measure based on a modified and weighted version of\nthe Tversky Index that accounts for synonymy, negation, and hierarchical\nrelationships between medical entities. This allows efficient and semantically\nmeaningful similarity comparisons between reports. We demonstrate that our\napproach outperforms state-of-the-art embedding-based retrieval methods in a\nradiograph classification task on MIMIC-CXR, particularly in long-tail\nsettings. Additionally, we use our pipeline to generate ontology-backed disease\nlabels for MIMIC-CXR, offering a valuable new resource for downstream learning\ntasks. Our work provides more explainable, reliable, and task-specific\nretrieval strategies in clinical AI systems, especially when interpretability\nand domain knowledge integration are essential. Our code is available at\nhttps://github.com/Felix-012/ontology-concept-distillation",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology\nAbstract: Foundation models have recently emerged as powerful feature extractors in\ncomputational pathology, yet they typically omit mechanisms for leveraging the\nglobal spatial structure of tissues and the local contextual relationships\namong diagnostically relevant regions - key elements for understanding the\ntumor microenvironment. Multiple instance learning (MIL) remains an essential\nnext step following foundation model, designing a framework to aggregate\npatch-level features into slide-level predictions. We present EAGLE-Net, a\nstructure-preserving, attention-guided MIL architecture designed to augment\nprediction and interpretability. EAGLE-Net integrates multi-scale absolute\nspatial encoding to capture global tissue architecture, a top-K\nneighborhood-aware loss to focus attention on local microenvironments, and\nbackground suppression loss to minimize false positives. We benchmarked\nEAGLE-Net on large pan-cancer datasets, including three cancer types for\nclassification (10,260 slides) and seven cancer types for survival prediction\n(4,172 slides), using three distinct histology foundation backbones (REMEDIES,\nUni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher\nclassification accuracy and the top concordance indices in 6 of 7 cancer types,\nproducing smooth, biologically coherent attention maps that aligned with expert\nannotations and highlighted invasive fronts, necrosis, and immune infiltration.\nThese results position EAGLE-Net as a generalizable, interpretable framework\nthat complements foundation models, enabling improved biomarker discovery,\nprognostic modeling, and clinical decision support",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission\nAbstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach\nintensity modulation and direct detection communication systems. However, their\ncomplex nonlinear dynamics make the modeling and optimization of DML-based\nsystems challenging. In this paper, we study the end-to-end optimization of\nDML-based systems based on a data-driven surrogate model trained on\nexperimental data. The end-to-end optimization includes the pulse shaping and\nequalizer filters, the bias current and the modulation radio-frequency (RF)\npower applied to the laser. The performance of the end-to-end optimization\nscheme is tested on the experimental setup and compared to 4 different\nbenchmark schemes based on linear and nonlinear receiver-side equalization. The\nresults show that the proposed end-to-end scheme is able to deliver better\nperformance throughout the studied symbol rates and transmission distances\nwhile employing lower modulation RF power, fewer filter taps and utilizing a\nsmaller signal bandwidth.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Streamlining the Development of Active Learning Methods in Real-World Object Detection\nAbstract: Active learning (AL) for real-world object detection faces computational and\nreliability challenges that limit practical deployment. Developing new AL\nmethods requires training multiple detectors across iterations to compare\nagainst existing approaches. This creates high costs for autonomous driving\ndatasets where the training of one detector requires up to 282 GPU hours.\nAdditionally, AL method rankings vary substantially across validation sets,\ncompromising reliability in safety-critical transportation systems. We\nintroduce object-based set similarity ($\\mathrm{OSS}$), a metric that addresses\nthese challenges. $\\mathrm{OSS}$ (1) quantifies AL method effectiveness without\nrequiring detector training by measuring similarity between training sets and\ntarget domains using object-level features. This enables the elimination of\nineffective AL methods before training. Furthermore, $\\mathrm{OSS}$ (2) enables\nthe selection of representative validation sets for robust evaluation. We\nvalidate our similarity-based approach on three autonomous driving datasets\n(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with\ntwo detector architectures (EfficientDet, YOLOv3). This work is the first to\nunify AL training and evaluation strategies in object detection based on object\nsimilarity. $\\mathrm{OSS}$ is detector-agnostic, requires only labeled object\ncrops, and integrates with existing AL pipelines. This provides a practical\nframework for deploying AL in real-world applications where computational\nefficiency and evaluation reliability are critical. Code is available at\nhttps://mos-ks.github.io/publications/.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs\nAbstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,\nthe goal of link sign prediction is to predict the signs of potential links\nconnecting U and V based on known positive and negative edges in G. The\nmajority of existing solutions towards link sign prediction mainly focus on\nunipartite signed graphs, which are sub-optimal due to the neglect of node\nheterogeneity and unique bipartite characteristics of SBGs. To this end, recent\nstudies adapt graph neural networks to SBGs by introducing message-passing\nschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node\npairs. However, the fundamental spectral convolutional operators were\noriginally designed for positive links in unsigned graphs, and thus, are not\noptimal for inferring missing positive or negative links from known ones in\nSBGs.\n  Motivated by this, this paper proposes GegenNet, a novel and effective\nspectral convolutional neural network model for link sign prediction in SBGs.\nIn particular, GegenNet achieves enhanced model capacity and high predictive\naccuracy through three main technical contributions: (i) fast and theoretically\ngrounded spectral decomposition techniques for node feature initialization;\n(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and\n(iii) multi-layer sign-aware spectral convolutional networks alternating\nGegenbauer polynomial filters with positive and negative edges. Our extensive\nempirical studies reveal that GegenNet can achieve significantly superior\nperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign\nprediction compared to 11 strong competitors over 6 benchmark SBG datasets.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning\nAbstract: Offline reinforcement learning (RL) enables learning effective policies from\nfixed datasets without any environment interaction. Existing methods typically\nemploy policy constraints to mitigate the distribution shift encountered during\noffline RL training. However, because the scale of the constraints varies\nacross tasks and datasets of differing quality, existing methods must\nmeticulously tune hyperparameters to match each dataset, which is\ntime-consuming and often impractical. We propose Adaptive Scaling of Policy\nConstraints (ASPC), a second-order differentiable framework that dynamically\nbalances RL and behavior cloning (BC) during training. We theoretically analyze\nits performance improvement guarantee. In experiments on 39 datasets across\nfour D4RL domains, ASPC using a single hyperparameter configuration outperforms\nother adaptive constraint methods and state-of-the-art offline RL algorithms\nthat require per-dataset tuning while incurring only minimal computational\noverhead. The code will be released at https://github.com/Colin-Jing/ASPC.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: The Information Dynamics of Generative Diffusion\nAbstract: Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This perspective paper provides an integrated perspective on\ngenerative diffusion by connecting their dynamic, information-theoretic, and\nthermodynamic properties under a unified mathematical framework. We demonstrate\nthat the rate of conditional entropy production during generation (i.e. the\ngenerative bandwidth) is directly governed by the expected divergence of the\nscore function's vector field. This divergence, in turn, is linked to the\nbranching of trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs\nAbstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos\nAbstract: Recent advances in motion generation show remarkable progress. However,\nseveral limitations remain: (1) Existing pose-guided character motion transfer\nmethods merely replicate motion without learning its style characteristics,\nresulting in inexpressive characters. (2) Motion style transfer methods rely\nheavily on motion capture data, which is difficult to obtain. (3) Generated\nmotions sometimes violate physical laws. To address these challenges, this\npaper pioneers a new task: Video-to-Video Motion Personalization. We propose a\nnovel framework, PersonaAnimator, which learns personalized motion patterns\ndirectly from unconstrained videos. This enables personalized motion transfer.\nTo support this task, we introduce PersonaVid, the first video-based\npersonalized motion dataset. It contains 20 motion content categories and 120\nmotion style categories. We further propose a Physics-aware Motion Style\nRegularization mechanism to enforce physical plausibility in the generated\nmotions. Extensive experiments show that PersonaAnimator outperforms\nstate-of-the-art motion transfer methods and sets a new benchmark for the\nVideo-to-Video Motion Personalization task.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Kullback-Leibler Potential for Non-Ergodic Replication Dynamics:An Information-Theoretic Second Law\nAbstract: This study aims to quantify and visualize the degradation of fidelity\n(information degradation) that inevitably accompanies the replication of\ninformation within the framework of information thermodynamics and to propose\nan information-theoretic formulation of the second law based on this\nphenomenon. While previous research in information thermodynamics has focused\non the thermodynamic costs associated with information \"erasure'' or\n\"measurement'' through concepts such as Landauer's principle and mutual\ninformation, little systematic discussion has addressed the inherently\nirreversible nature of \"replication'' itself and the accompanying degradation\nof information structure. In this study, we construct a mathematical model of\ninformation replication using a discrete Markov model and Gaussian convolution,\nand quantify changes in information at each replication step: Shannon entropy,\ncross-entropy, and the Kullback--Leibler divergence (KLD). The monotonic\ndecrease of KLD exhibits a Lyapunov-like property, which can be interpreted as\na potential analogous to the free energy in the process by which a\nnonequilibrium system converges to a particular steady state. Furthermore, we\nextend this framework to the potential applicability to biological information\nprocesses such as DNA replication, showing that the free energy required for\ndegradation and repair can be expressed in terms of KLD. This contributes to\nbuilding a unified information-thermodynamic framework for operations such as\nreplication, transmission, and repair of information.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement\nAbstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks\nAbstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured\ndata modeling tasks such as node classification. However, mainstream approaches\ngenerally rely on a large number of trainable parameters and fixed aggregation\nrules, making it difficult to adapt to graph data with strong structural\nheterogeneity and complex feature distributions. This often leads to\nover-smoothing of node representations and semantic degradation. To address\nthese issues, this paper proposes a parameter-free graph neural network\nframework based on structural diversity, namely SDGNN (Structural-Diversity\nGraph Neural Network). The framework is inspired by structural diversity theory\nand designs a unified structural-diversity message passing mechanism that\nsimultaneously captures the heterogeneity of neighborhood structures and the\nstability of feature semantics, without introducing additional trainable\nparameters. Unlike traditional parameterized methods, SDGNN does not rely on\ncomplex model training, but instead leverages complementary modeling from both\nstructure-driven and feature-driven perspectives, thereby effectively improving\nadaptability across datasets and scenarios. Experimental results show that on\neight public benchmark datasets and an interdisciplinary PubMed citation\nnetwork, SDGNN consistently outperforms mainstream GNNs under challenging\nconditions such as low supervision, class imbalance, and cross-domain transfer.\nThis work provides a new theoretical perspective and general approach for the\ndesign of parameter-free graph neural networks, and further validates the\nimportance of structural diversity as a core signal in graph representation\nlearning. To facilitate reproducibility and further research, the full\nimplementation of SDGNN has been released at:\nhttps://github.com/mingyue15694/SGDNN/tree/main",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: AI-Powered Detection of Inappropriate Language in Medical School Curricula\nAbstract: The use of inappropriate language -- such as outdated, exclusionary, or\nnon-patient-centered terms -- medical instructional materials can significantly\ninfluence clinical training, patient interactions, and health outcomes. Despite\ntheir reputability, many materials developed over past decades contain examples\nnow considered inappropriate by current medical standards. Given the volume of\ncurricular content, manually identifying instances of inappropriate use of\nlanguage (IUL) and its subcategories for systematic review is prohibitively\ncostly and impractical. To address this challenge, we conduct a first-in-class\nevaluation of small language models (SLMs) fine-tuned on labeled data and\npre-trained LLMs with in-context learning on a dataset containing approximately\n500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL\nclassifier, (2) subcategory-specific binary classifiers, (3) a multilabel\nclassifier, and (4) a two-stage hierarchical pipeline for general IUL detection\nfollowed by multilabel classification. For LLMs, we consider variations of\nprompts that include subcategory definitions and/or shots. We found that both\nLLama-3 8B and 70B, even with carefully curated shots, are largely outperformed\nby SLMs. While the multilabel classifier performs best on annotated data,\nsupplementing training with unflagged excerpts as negative examples boosts the\nspecific classifiers' AUC by up to 25%, making them most effective models for\nmitigating harmful language in medical curricula.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Multispectral LiDAR data for extracting tree points in urban and suburban areas\nAbstract: Monitoring urban tree dynamics is vital for supporting greening policies and\nreducing risks to electrical infrastructure. Airborne laser scanning has\nadvanced large-scale tree management, but challenges remain due to complex\nurban environments and tree variability. Multispectral (MS) light detection and\nranging (LiDAR) improves this by capturing both 3D spatial and spectral data,\nenabling detailed mapping. This study explores tree point extraction using\nMS-LiDAR and deep learning (DL) models. Three state-of-the-art models are\nevaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point\nTransformer V1 (PTv1). Results show the notable time efficiency and accuracy of\nSPT, with a mean intersection over union (mIoU) of 85.28%. The highest\ndetection accuracy is achieved by incorporating pseudo normalized difference\nvegetation index (pNDVI) with spatial data, reducing error rate by 10.61\npercentage points (pp) compared to using spatial information alone. These\nfindings highlight the potential of MS-LiDAR and DL to improve tree extraction\nand further tree inventories.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: On-chip wave chaos for photonic extreme learning\nAbstract: The increase in demand for scalable and energy efficient artificial neural\nnetworks has put the focus on novel hardware solutions. Integrated photonics\noffers a compact, parallel and ultra-fast information processing platform,\nspecially suited for extreme learning machine (ELM) architectures. Here we\nexperimentally demonstrate a chip-scale photonic ELM based on wave chaos\ninterference in a stadium microcavity. By encoding the input information in the\nwavelength of an external single-frequency tunable laser source, we leverage\nthe high sensitivity to wavelength of injection in such photonic resonators. We\nfabricate the microcavity with direct laser writing of SU-8 polymer on glass. A\nscattering wall surrounding the stadium operates as readout layer, collecting\nthe light associated with the cavity's leaky modes. We report uncorrelated and\naperiodic behavior in the speckles of the scattering barrier from a high\nresolution scan of the input wavelength. Finally, we characterize the system's\nperformance at classification in four qualitatively different benchmark tasks.\nAs we can control the number of output nodes of our ELM by measuring different\nparts of the scattering barrier, we demonstrate the capability to optimize our\nphotonic ELM's readout size to the performance required for each task.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: The IRMA Dataset: A Structured Audio-MIDI Corpus for Iranian Classical Music\nAbstract: We present the IRMA Dataset (Iranian Radif MIDI Audio), a multi-level,\nopen-access corpus designed for the computational study of Iranian classical\nmusic, with a particular emphasis on the radif, a structured repertoire of\nmodal-melodic units central to pedagogy and performance. The dataset combines\nsymbolic MIDI representations, phrase-level audio-MIDI alignment, musicological\ntranscriptions in PDF format, and comparative tables of theoretical information\ncurated from a range of performers and scholars. We outline the multi-phase\nconstruction process, including segment annotation, alignment methods, and a\nstructured system of identifier codes to reference individual musical units.\nThe current release includes the complete radif of Karimi; MIDI files and\nmetadata from Mirza Abdollah's radif; selected segments from the vocal radif of\nDavami, as transcribed by Payvar and Fereyduni; and a dedicated section\nfeaturing audio-MIDI examples of tahrir ornamentation performed by prominent\n20th-century vocalists. While the symbolic and analytical components are\nreleased under an open-access license (CC BY-NC 4.0), some referenced audio\nrecordings and third-party transcriptions are cited using discographic\ninformation to enable users to locate the original materials independently,\npending copyright permission. Serving both as a scholarly archive and a\nresource for computational analysis, this dataset supports applications in\nethnomusicology, pedagogy, symbolic audio research, cultural heritage\npreservation, and AI-driven tasks such as automatic transcription and music\ngeneration. We welcome collaboration and feedback to support its ongoing\nrefinement and broader integration into musicological and machine learning\nworkflows.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network\nAbstract: Sky background subtraction is a critical step in Multi-objective Fiber\nspectra process. However, current subtraction relies mainly on sky fiber\nspectra to build Super Sky. These average spectra are lacking in the modeling\nof the environment surrounding the objects. To address this issue, a sky\nbackground estimation model: Sky background building based on Mutual\nInformation (SMI) is proposed. SMI based on mutual information and incremental\ntraining approach. It utilizes spectra from all fibers in the plate to estimate\nthe sky background. SMI contains two main networks, the first network applies a\nwavelength calibration module to extract sky features from spectra, and can\neffectively solve the feature shift problem according to the corresponding\nemission position. The second network employs an incremental training approach\nto maximize mutual information between representations of different spectra to\ncapturing the common component. Then, it minimizes the mutual information\nbetween adjoining spectra representations to obtain individual components. This\nnetwork yields an individual sky background at each location of the object. To\nverify the effectiveness of the method in this paper, we conducted experiments\non the spectra of LAMOST. Results show that SMI can obtain a better object sky\nbackground during the observation, especially in the blue end.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning\nAbstract: Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations\nAbstract: With the introduction of vehicles with autonomous capabilities on public\nroads, predicting pedestrian crossing intention has emerged as an active area\nof research. The task of predicting pedestrian crossing intention involves\ndetermining whether pedestrians in the scene are likely to cross the road or\nnot. In this work, we propose TrajFusionNet, a novel transformer-based model\nthat combines future pedestrian trajectory and vehicle speed predictions as\npriors for predicting crossing intention. TrajFusionNet comprises two branches:\na Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM\nbranch learns from a sequential representation of the observed and predicted\npedestrian trajectory and vehicle speed. Complementarily, the VAM branch\nenables learning from a visual representation of the predicted pedestrian\ntrajectory by overlaying predicted pedestrian bounding boxes onto scene images.\nBy utilizing a small number of lightweight modalities, TrajFusionNet achieves\nthe lowest total inference time (including model runtime and data\npreprocessing) among current state-of-the-art approaches. In terms of\nperformance, it achieves state-of-the-art results across the three most\ncommonly used datasets for pedestrian crossing intention prediction.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Self-supervised structured object representation learning\nAbstract: Self-supervised learning (SSL) has emerged as a powerful technique for\nlearning visual representations. While recent SSL approaches achieve strong\nresults in global image understanding, they are limited in capturing the\nstructured representation in scenes. In this work, we propose a self-supervised\napproach that progressively builds structured visual representations by\ncombining semantic grouping, instance level separation, and hierarchical\nstructuring. Our approach, based on a novel ProtoScale module, captures visual\nelements across multiple spatial scales. Unlike common strategies like DINO\nthat rely on random cropping and global embeddings, we preserve full scene\ncontext across augmented views to improve performance in dense prediction\ntasks. We validate our method on downstream object detection tasks using a\ncombined subset of multiple datasets (COCO and UA-DETRAC). Experimental results\nshow that our method learns object centric representations that enhance\nsupervised object detection and outperform the state-of-the-art methods, even\nwhen trained with limited annotated data and fewer fine-tuning epochs.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction\nAbstract: Personalized, accurate prediction of aortic aneurysm progression is essential\nfor timely intervention but remains challenging due to the need to model both\nsubtle local deformations and global anatomical changes within complex 3D\ngeometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh\ngenerative adversarial network for 3D aneurysm growth prediction. MCMeshGAN\nintroduces a dual-branch architecture combining a novel local KNN-based\nconvolutional network (KCN) to preserve fine-grained geometric details and a\nglobal graph convolutional network (GCN) to capture long-range structural\ncontext, overcoming the over-smoothing limitations of deep GCNs. A dedicated\ncondition branch encodes clinical attributes (age, sex) and the target time\ninterval to generate anatomically plausible, temporally controlled predictions,\nenabling retrospective and prospective modeling. We curated TAAMesh, a new\nlongitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal\nrecords (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive\nexperiments demonstrate that MCMeshGAN consistently outperforms\nstate-of-the-art baselines in both geometric accuracy and clinically important\ndiameter estimation. This framework offers a robust step toward clinically\ndeployable, personalized 3D disease trajectory modeling. The source code for\nMCMeshGAN and the baseline methods is publicly available at\nhttps://github.com/ImperialCollegeLondon/MCMeshGAN.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Fractal analysis of slow-fast and regular systems: A survey of recent results and future perspectives\nAbstract: We survey recent developments in fractal analysis of regular and slow-fast\ndynamical systems using Minkowski dimension. Our focus is on spiral\ntrajectories near monodromic limit periodic sets in regular systems and\nentry-exit sequences in slow-fast systems with degenerate singularities. For\nregular systems, we recall connections between Minkowski dimension and\ncyclicity. Key results include fractal classifications of weak foci, degenerate\nfoci, and polycycles, where dimensional relationships predict limit cycle\nbirth. For slow-fast systems, we survey the coordinate-free fractal methodology\nanalyzing slow-fast Hopf points and canard cycles through slow divergence\nintegrals and entry-exit sequences. The Minkowski dimension takes discrete\nvalues yielding upper bounds for the number of limit cycles without normal form\ntransformations. The fractal approach provides computational advantages, works\nwith original coordinates, and reveals geometric structures underlying\nbifurcation phenomena. Applications span neuroscience, chemistry, population\ndynamics, and climate modeling. We also discuss extensions to piecewise smooth\nand three-dimensional systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Quantum latent distributions in deep generative models\nAbstract: Many successful families of generative models leverage a low-dimensional\nlatent distribution that is mapped to a data distribution. Though simple latent\ndistributions are commonly used, it has been shown that more sophisticated\ndistributions can improve performance. For instance, recent work has explored\nusing the distributions produced by quantum processors and found empirical\nimprovements. However, when latent space distributions produced by quantum\nprocessors can be expected to improve performance, and whether these\nimprovements are reproducible, are open questions that we investigate in this\nwork. We prove that, under certain conditions, these \"quantum latent\ndistributions\" enable generative models to produce data distributions that\nclassical latent distributions cannot efficiently produce. We also provide\nactionable intuitions to identify when such quantum advantages may arise in\nreal-world settings. We perform benchmarking experiments on both a synthetic\nquantum dataset and the QM9 molecular dataset, using both simulated and real\nphotonic quantum processors. Our results demonstrate that quantum latent\ndistributions can lead to improved generative performance in GANs compared to a\nrange of classical baselines. We also explore diffusion and flow matching\nmodels, identifying architectures compatible with quantum latent distributions.\nThis work confirms that near-term quantum processors can expand the\ncapabilities of deep generative models.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation\nAbstract: Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models\nAbstract: Machine vision systems (MVS) are intrinsically vulnerable to performance\ndegradation under adverse visual conditions. To address this, we propose a\nmachine-centric image quality assessment (MIQA) framework that quantifies the\nimpact of image degradations on MVS performance. We establish an MIQA paradigm\nencompassing the end-to-end assessment workflow. To support this, we construct\na machine-centric image quality database (MIQD-2.5M), comprising 2.5 million\nsamples that capture distinctive degradation responses in both consistency and\naccuracy metrics, spanning 75 vision models, 250 degradation types, and three\nrepresentative vision tasks. We further propose a region-aware MIQA (RA-MIQA)\nmodel to evaluate MVS visual quality through fine-grained spatial degradation\nanalysis. Extensive experiments benchmark the proposed RA-MIQA against seven\nhuman visual system (HVS)-based IQA metrics and five retrained classical\nbackbones. Results demonstrate RA-MIQA's superior performance in multiple\ndimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on\naccuracy for image classification, while also revealing task-specific\ndegradation sensitivities. Critically, HVS-based metrics prove inadequate for\nMVS quality prediction, while even specialized MIQA models struggle with\nbackground degradations, accuracy-oriented estimation, and subtle distortions.\nThis study can advance MVS reliability and establish foundations for\nmachine-centric image processing and optimization. The model and code are\navailable at: https://github.com/XiaoqiWang/MIQA.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources\nAbstract: We present a hybrid framework that couples finite element methods (FEM) with\nphysics-informed DeepONet to model fluid transport in porous media from sharp,\nlocalized Gaussian sources. The governing system consists of a steady-state\nDarcy flow equation and a time-dependent convection-diffusion equation. Our\napproach solves the Darcy system using FEM and transfers the resulting velocity\nfield to a physics-informed DeepONet, which learns the mapping from source\nfunctions to solute concentration profiles. This modular strategy preserves\nFEM-level accuracy in the flow field while enabling fast inference for\ntransport dynamics. To handle steep gradients induced by sharp sources, we\nintroduce an adaptive sampling strategy for trunk collocation points. Numerical\nexperiments demonstrate that our method is in good agreement with the reference\nsolutions while offering orders of magnitude speedups over traditional solvers,\nmaking it suitable for practical applications in relevant scenarios.\nImplementation of our proposed method is available at\nhttps://github.com/erkara/fem-pi-deeponet.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Exotic rheology of materials with active rearrangements\nAbstract: The flow of biological tissues during development is controlled through the\nactive stresses generated by cells interacting with their mechanical\nenvironment in the tissue. Many developmental processes are driven by\nconvergence-extension flows where the tissue has an emergent negative shear\nmodulus and viscosity. This exotic rheology is generated through active T1\ntransitions where rearrangements are opposite the applied stress direction.\nHere, we introduce a mean-field elasto-plastic model which shows\nconvergence-extension, based on the Hebraud-Lequeux model, that includes both\npassive and active elastic elements with opposite stress responses. We find\nthat the introduction of active elements profoundly changes the rheology.\nBeyond a threshold fraction of active elements, it gives rise to non-monotonic\nflow curves and negative stresses at positive strain rates. Controlled by the\nactive fraction and the stress diffusion rate, we find both yield stress\nmaterials and fluids, with either a positive or negative yield stress or\nviscosity. These features are characteristic of metamaterials, and highlight\nhow biology uses disordered, active materials with exotic rheology.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Symplectic convolutional neural networks\nAbstract: We propose a new symplectic convolutional neural network (CNN) architecture\nby leveraging symplectic neural networks, proper symplectic decomposition, and\ntensor techniques. Specifically, we first introduce a mathematically equivalent\nform of the convolution layer and then, using symplectic neural networks, we\ndemonstrate a way to parameterize the layers of the CNN to ensure that the\nconvolution layer remains symplectic. To construct a complete autoencoder, we\nintroduce a symplectic pooling layer. We demonstrate the performance of the\nproposed neural network on three examples: the wave equation, the nonlinear\nSchr\\\"odinger (NLS) equation, and the sine-Gordon equation. The numerical\nresults indicate that the symplectic CNN outperforms the linear symplectic\nautoencoder obtained via proper symplectic decomposition.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers\nAbstract: We present a probabilistic, data-driven surrogate model for predicting the\nradiative properties of nanoparticle embedded scattering media. The model uses\nconditional normalizing flows, which learn the conditional distribution of\noptical outputs, including reflectance, absorbance, and transmittance, given\ninput parameters such as the absorption coefficient, scattering coefficient,\nanisotropy factor, and particle size distribution. We generate training data\nusing Monte Carlo radiative transfer simulations, with optical properties\nderived from Mie theory. Unlike conventional neural networks, the conditional\nnormalizing flow model yields full posterior predictive distributions, enabling\nboth accurate forecasts and principled uncertainty quantification. Our results\ndemonstrate that this model achieves high predictive accuracy and reliable\nuncertainty estimates, establishing it as a powerful and efficient surrogate\nfor radiative transfer simulations.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: PSO-Merging: Merging Models Based on Particle Swarm Optimization\nAbstract: Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis\nAbstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Gradient Rectification for Robust Calibration under Distribution Shift\nAbstract: Deep neural networks often produce overconfident predictions, undermining\ntheir reliability in safety-critical applications. This miscalibration is\nfurther exacerbated under distribution shift, where test data deviates from the\ntraining distribution due to environmental or acquisition changes. While\nexisting approaches improve calibration through training-time regularization or\npost-hoc adjustment, their reliance on access to or simulation of target\ndomains limits their practicality in real-world scenarios. In this paper, we\npropose a novel calibration framework that operates without access to target\ndomain information. From a frequency-domain perspective, we identify that\ndistribution shifts often distort high-frequency visual cues exploited by deep\nmodels, and introduce a low-frequency filtering strategy to encourage reliance\non domain-invariant features. However, such information loss may degrade\nIn-Distribution (ID) calibration performance. Therefore, we further propose a\ngradient-based rectification mechanism that enforces ID calibration as a hard\nconstraint during optimization. Experiments on synthetic and real-world shifted\ndatasets, including CIFAR-10/100-C and WILDS, demonstrate that our method\nsignificantly improves calibration under distribution shift while maintaining\nstrong in-distribution performance.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning\nAbstract: Large Language Models (LLMs) have demonstrated impressive capabilities across\na wide range of NLP tasks, but they remain fundamentally stateless, constrained\nby limited context windows that hinder long-horizon reasoning. Recent efforts\nto address this limitation often augment LLMs with an external memory bank, yet\nmost existing pipelines are static and heuristic-driven, lacking any learned\nmechanism for deciding what to store, update, or retrieve. We present\nMemory-R1, a reinforcement learning (RL) framework that equips LLMs with the\nability to actively manage and utilize external memory through two specialized\nagents: a Memory Manager that learns to perform structured memory operations\n{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant\nentries and reasons over them to produce an answer. Both agents are fine-tuned\nwith outcome-driven RL (PPO and GRPO), enabling adaptive memory management and\nuse with minimal supervision. With as few as 152 question-answer pairs and a\ncorresponding temporal memory bank for training, Memory-R1 outperforms the most\ncompetitive existing baseline and demonstrates strong generalization across\ndiverse question types and LLM backbones. Beyond presenting an effective\napproach, this work provides insights into how RL can unlock more agentic,\nmemory-aware behaviors in LLMs, pointing toward richer, more persistent\nreasoning systems.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: A Constructed Closure of the Bering Strait can Prevent an AMOC Tipping\nAbstract: The Atlantic Meridional Overturning Circulation (AMOC) is a major tipping\nelement in the present-day climate, and could potentially collapse under\nsufficient freshwater or CO2-forcing. While the effect of the Bering Strait on\nAMOC stability has been well studied, it is unknown whether a constructed\nclosure of this Strait can prevent an AMOC collapse under climate change. Here,\nwe show in an Earth system Model of Intermediate Complexity that an artificial\nclosure of the Strait can extend the safe carbon budget of the AMOC, provided\nthat the AMOC is strong enough at the closure time. Specifically, for this\nmodel, an equilibrium AMOC with a reduction below (6.1+/-0.5)% from\npre-industrial has an additional budget up to 500PgC given a sufficiently early\nclosure, while for a weaker AMOC a closure reduces this budget. This indicates\nthat constructing this closure can be a feasible climate intervention strategy\nto prevent an AMOC collapse.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping\nAbstract: The privacy community has a long track record of investigating emerging types\nof web tracking techniques. Recent work has focused on compliance of web\ntrackers with new privacy laws such as Europe's GDPR and California's CCPA.\nDespite the growing body of research documenting widespread lack of compliance\nwith new privacy laws, there is a lack of robust enforcement. Different from\nprior work, we conduct a tech-law analysis to map decades-old U.S. laws about\ninterception of electronic communications--so-called wiretapping--to web\ntracking. Bridging the tech-law gap for older wiretapping laws is important and\ntimely because, in cases where legal harm to privacy is proven, they can\nprovide statutory private right of action, are at the forefront of recent\nprivacy enforcement, and could ultimately lead to a meaningful change in the\nweb tracking landscape.\n  In this paper, we focus on a particularly invasive tracking technique: the\nuse of JavaScript event listeners by third-party trackers for real-time\nkeystroke interception on websites. We use an instrumented web browser to crawl\na sample of the top-million websites to investigate the use of event listeners\nthat aligns with the criteria for wiretapping, according to U.S. wiretapping\nlaw at the federal level and in California. We find evidence that 38.52%\nwebsites installed third-party event listeners to intercept keystrokes, and\nthat at least 3.18% websites transmitted intercepted information to a\nthird-party server, which aligns with the criteria for wiretapping. We further\nfind evidence that the intercepted information such as email addresses typed\ninto form fields are used for unsolicited email marketing. Beyond our work that\nmaps the intersection between technical measurement and U.S. wiretapping law,\nadditional future legal research is required to determine when the wiretapping\nobserved in our paper passes the threshold for illegality.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning\nAbstract: Gradient inversion attacks have garnered attention for their ability to\ncompromise privacy in federated learning. However, many studies consider\nattacks with the model in inference mode, where training-time behaviors like\ndropout are disabled and batch normalization relies on fixed statistics. In\nthis work, we systematically analyze how architecture and training behavior\naffect vulnerability, including the first in-depth study of inference-mode\nclients, which we show dramatically simplifies inversion. To assess attack\nfeasibility under more realistic conditions, we turn to clients operating in\nstandard training mode. In this setting, we find that successful attacks are\nonly possible when several architectural conditions are met simultaneously:\nmodels must be shallow and wide, use skip connections, and, critically, employ\npre-activation normalization. We introduce two novel attacks against models in\ntraining-mode with varying attacker knowledge, achieving state-of-the-art\nperformance under realistic training conditions. We extend these efforts by\npresenting the first attack on a production-grade object-detection model. Here,\nto enable any visibly identifiable leakage, we revert to the lenient inference\nmode setting and make multiple architectural modifications to increase model\nvulnerability, with the extent of required changes highlighting the strong\ninherent robustness of such architectures. We conclude this work by offering\nthe first comprehensive mapping of settings, clarifying which combinations of\narchitectural choices and operational modes meaningfully impact privacy. Our\nanalysis provides actionable insight into when models are likely vulnerable,\nwhen they appear robust, and where subtle leakage may persist. Together, these\nfindings reframe how gradient inversion risk should be assessed in future\nresearch and deployment scenarios.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Towards a Real-Time Warning System for Detecting Inaccuracies in Photoplethysmography-Based Heart Rate Measurements in Wearable Devices\nAbstract: Wearable devices with photoplethysmography (PPG) sensors are widely used to\nmonitor heart rate (HR), yet often suffer from accuracy issues. However, users\ntypically do not receive an indication of potential measurement errors. We\npresent a real-time warning system that detects and communicates inaccuracies\nin PPG-derived HR, aiming to enhance transparency and trust. Using data from\nPolar and Garmin devices, we trained a deep learning model to classify HR\naccuracy using only the derived HR signal. The system detected over 80% of\ninaccurate readings. By providing interpretable, real-time feedback directly to\nusers, our work contributes to HCI by promoting user awareness, informed\ndecision-making, and trust in wearable health technology.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables\nAbstract: Extensive research has been conducted to explore the capabilities of large\nlanguage models (LLMs) in table reasoning. However, the essential task of\ntransforming tables information into reports remains a significant challenge\nfor industrial applications. This task is plagued by two critical issues: 1)\nthe complexity and diversity of tables lead to suboptimal reasoning outcomes;\nand 2) existing table benchmarks lack the capacity to adequately assess the\npractical application of this task. To fill this gap, we propose the\ntable-to-report task and construct a bilingual benchmark named T2R-bench, where\nthe key information flow from the tables to the reports for this task. The\nbenchmark comprises 457 industrial tables, all derived from real-world\nscenarios and encompassing 19 industry domains as well as 4 types of industrial\ntables. Furthermore, we propose an evaluation criteria to fairly measure the\nquality of report generation. The experiments on 25 widely-used LLMs reveal\nthat even state-of-the-art models like Deepseek-R1 only achieves performance\nwith 62.71 overall score, indicating that LLMs still have room for improvement\non T2R-bench. Source code and data will be available after acceptance.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Bootstrapping Learned Cost Models with Synthetic SQL Queries\nAbstract: Having access to realistic workloads for a given database instance is\nextremely important to enable stress and vulnerability testing, as well as to\noptimize for cost and performance. Recent advances in learned cost models have\nshown that when enough diverse SQL queries are available, one can effectively\nand efficiently predict the cost of running a given query against a specific\ndatabase engine. In this paper, we describe our experience in exploiting modern\nsynthetic data generation techniques, inspired by the generative AI and LLM\ncommunity, to create high-quality datasets enabling the effective training of\nsuch learned cost models. Initial results show that we can improve a learned\ncost model's predictive accuracy by training it with 45% fewer queries than\nwhen using competitive generation approaches.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Context-aware Sparse Spatiotemporal Learning for Event-based Vision\nAbstract: Event-based camera has emerged as a promising paradigm for robot perception,\noffering advantages with high temporal resolution, high dynamic range, and\nrobustness to motion blur. However, existing deep learning-based event\nprocessing methods often fail to fully leverage the sparse nature of event\ndata, complicating their integration into resource-constrained edge\napplications. While neuromorphic computing provides an energy-efficient\nalternative, spiking neural networks struggle to match of performance of\nstate-of-the-art models in complex event-based vision tasks, like object\ndetection and optical flow. Moreover, achieving high activation sparsity in\nneural networks is still difficult and often demands careful manual tuning of\nsparsity-inducing loss terms. Here, we propose Context-aware Sparse\nSpatiotemporal Learning (CSSL), a novel framework that introduces context-aware\nthresholding to dynamically regulate neuron activations based on the input\ndistribution, naturally reducing activation density without explicit sparsity\nconstraints. Applied to event-based object detection and optical flow\nestimation, CSSL achieves comparable or superior performance to\nstate-of-the-art methods while maintaining extremely high neuronal sparsity.\nOur experimental results highlight CSSL's crucial role in enabling efficient\nevent-based vision for neuromorphic processing.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: A Multi-Layered Framework for Modeling Human Biology: From Basic AI Agents to a Full-Body AI Agent\nAbstract: We envision the Full-Body AI Agent as a comprehensive AI system designed to\nsimulate, analyze, and optimize the dynamic processes of the human body across\nmultiple biological levels. By integrating computational models, machine\nlearning tools, and experimental platforms, this system aims to replicate and\npredict both physiological and pathological processes, ranging from molecules\nand cells to tissues, organs, and entire body systems. Central to the Full-Body\nAI Agent is its emphasis on integration and coordination across these\nbiological levels, enabling analysis of how molecular changes influence\ncellular behaviors, tissue responses, organ function, and systemic outcomes.\nWith a focus on biological functionality, the system is designed to advance the\nunderstanding of disease mechanisms, support the development of therapeutic\ninterventions, and enhance personalized medicine. We propose two specialized\nimplementations to demonstrate the utility of this framework: (1) the\nmetastasis AI Agent, a multi-scale metastasis scoring system that characterizes\ntumor progression across the initiation, dissemination, and colonization phases\nby integrating molecular, cellular, and systemic signals; and (2) the drug AI\nAgent, a system-level drug development paradigm in which a drug AI-Agent\ndynamically guides preclinical evaluations, including organoids and chip-based\nmodels, by providing full-body physiological constraints. This approach enables\nthe predictive modeling of long-term efficacy and toxicity beyond what\nlocalized models alone can achieve. These two agents illustrate the potential\nof Full-Body AI Agent to address complex biomedical challenges through\nmulti-level integration and cross-scale reasoning.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction\nAbstract: 3D Gaussian Splatting, known for enabling high-quality static scene\nreconstruction with fast rendering, is increasingly being applied to dynamic\nscene reconstruction. A common strategy involves learning a deformation field\nto model the temporal changes of a canonical set of 3D Gaussians. However,\nthese deformation-based methods often produce blurred renderings and lose fine\nmotion details in highly dynamic regions due to the inherent limitations of a\nsingle, unified model in representing diverse motion patterns. To address these\nchallenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian\nSplatting (MAPo), a novel framework for high-fidelity dynamic scene\nreconstruction. Its core is a dynamic score-based partitioning strategy that\ndistinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D\nGaussians, we recursively partition them temporally and duplicate their\ndeformation networks for each new temporal segment, enabling specialized\nmodeling to capture intricate motion details. Concurrently, low-dynamic 3DGs\nare treated as static to reduce computational costs. However, this temporal\npartitioning strategy for high-dynamic 3DGs can introduce visual\ndiscontinuities across frames at the partition boundaries. To address this, we\nintroduce a cross-frame consistency loss, which not only ensures visual\ncontinuity but also further enhances rendering quality. Extensive experiments\ndemonstrate that MAPo achieves superior rendering quality compared to baselines\nwhile maintaining comparable computational costs, particularly in regions with\ncomplex or rapid motions.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    },
    {
      "text": "Title: Search for thermodynamically stable ambient-pressure superconducting hydrides in GNoME database\nAbstract: Hydrides are considered to be one of the most promising families of compounds\nfor achieving high temperature superconductivity. However, there are very few\nexperimental reports of ambient-pressure hydride superconductivity, and the\nsuperconducting critical temperatures ($T_{\\rm c}$) are typically less than 10\nK. At the same time several hydrides have been predicted to exhibit\nsuperconductivity around 100 K at ambient pressure but in thermodynamically\nunfavorable phases. In this work we aim at assessing the superconducting\nproperties of thermodynamically stable hydride superconductors at room pressure\nby investigating the GNoME material database, which has been recently released\nand includes thousands of thermodynamically stable hydrides. To scan this large\nmaterial space we have adopted a multi stage approach which combines machine\nlearning for a fast initial evaluation and cutting edge ab initio methods to\nobtain a reliable estimation of $T_{\\rm c}$. Ultimately we have identified 22\ncubic thermodynamically stable hydrides with $T_{\\rm c}$ above 4.2 K and reach\na maximum $T_{\\rm c}$ of 17 K. While these critical temperatures are modest in\ncomparison to some recent predictions, the systems where they are found, being\nstable, are likely to be experimentally accessible and of potential\ntechnological relevance.",
      "source_type": "scientific",
      "credibility": 0.9,
      "is_factual": true
    }
  ]
}