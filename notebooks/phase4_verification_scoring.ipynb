{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8865be1",
   "metadata": {},
   "source": [
    "# Phase 4 — Verification & Scoring\n",
    "\n",
    "This notebook tests and debugs the verification and scoring pipeline:\n",
    "- NLI/Stance model → SUPPORTED / REFUTED / UNCLEAR\n",
    "- Show 3 citations (with highlights)\n",
    "- Confidence badge: 🟢 Likely True (≥0.7) | 🟡 Unclear (0.4–0.7) | 🔴 Likely False (≤0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e53c9",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3110ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "# Define verdict types\n",
    "class Verdict(Enum):\n",
    "    SUPPORTED = \"SUPPORTED\"\n",
    "    REFUTED = \"REFUTED\"\n",
    "    UNCLEAR = \"UNCLEAR\"\n",
    "\n",
    "# Test data from Phase 3\n",
    "test_claim_evidence = {\n",
    "    'claim': 'The new COVID vaccine causes severe side effects in 80% of patients',\n",
    "    'evidence_sources': [\n",
    "        {\n",
    "            'title': 'PIB Fact Check: No Evidence of 80% Severe Side Effects from COVID Vaccines',\n",
    "            'content': 'PIB fact-checking unit debunks false claims about COVID vaccine side effects. Clinical trials show side effects occur in less than 5% of patients and are mostly mild.',\n",
    "            'source_name': 'Press Information Bureau (PIB)',\n",
    "            'credibility_score': 0.95,\n",
    "            'key_snippets': ['Clinical trials show side effects occur in less than 5% of patients and are mostly mild']\n",
    "        },\n",
    "        {\n",
    "            'title': 'WHO Updates COVID-19 Vaccine Safety Guidelines',\n",
    "            'content': 'WHO releases new guidelines on COVID-19 vaccine safety monitoring. Studies confirm vaccines are safe with rare serious adverse events.',\n",
    "            'source_name': 'World Health Organization',\n",
    "            'credibility_score': 0.95,\n",
    "            'key_snippets': ['Studies confirm vaccines are safe with rare serious adverse events']\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Dependencies loaded successfully!\")\n",
    "print(f\"Test claim: {test_claim_evidence['claim']}\")\n",
    "print(f\"Evidence sources: {len(test_claim_evidence['evidence_sources'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93330a2c",
   "metadata": {},
   "source": [
    "## Step 2: Natural Language Inference (NLI) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b464234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nli_model(model_name: str = \"microsoft/DialoGPT-medium\"):\n",
    "    \"\"\"Load NLI model for stance detection\"\"\"\n",
    "    try:\n",
    "        # For demo, we'll use a simple rule-based approach\n",
    "        # In production, use models like: \n",
    "        # - \"microsoft/DialoGPT-medium\"\n",
    "        # - \"facebook/bart-large-mnli\"\n",
    "        # - \"roberta-large-mnli\"\n",
    "        \n",
    "        print(\"Loading NLI model (mocked for demo)...\")\n",
    "        return \"mock_nli_model\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading NLI model: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def mock_nli_inference(premise: str, hypothesis: str) -> Dict[str, float]:\n",
    "    \"\"\"Mock NLI inference for demonstration\"\"\"\n",
    "    # Simple rule-based logic for demo\n",
    "    premise_lower = premise.lower()\n",
    "    hypothesis_lower = hypothesis.lower()\n",
    "    \n",
    "    # Look for contradictory keywords\n",
    "    contradictory_pairs = [\n",
    "        (['80%', 'severe'], ['5%', 'mild', 'rare']),\n",
    "        (['causes', 'harmful'], ['safe', 'effective']),\n",
    "        (['dangerous', 'toxic'], ['approved', 'beneficial'])\n",
    "    ]\n",
    "    \n",
    "    # Check for contradictions\n",
    "    contradiction_score = 0\n",
    "    for claim_words, evidence_words in contradictory_pairs:\n",
    "        claim_match = any(word in hypothesis_lower for word in claim_words)\n",
    "        evidence_match = any(word in premise_lower for word in evidence_words)\n",
    "        if claim_match and evidence_match:\n",
    "            contradiction_score += 0.3\n",
    "    \n",
    "    # Calculate scores\n",
    "    if contradiction_score > 0.5:\n",
    "        return {\n",
    "            'CONTRADICTION': 0.8,\n",
    "            'ENTAILMENT': 0.1,\n",
    "            'NEUTRAL': 0.1\n",
    "        }\n",
    "    elif contradiction_score > 0.2:\n",
    "        return {\n",
    "            'CONTRADICTION': 0.4,\n",
    "            'ENTAILMENT': 0.2,\n",
    "            'NEUTRAL': 0.4\n",
    "        }\n",
    "    else:\n",
    "        # Look for supporting evidence\n",
    "        support_keywords = ['confirms', 'shows', 'proves', 'evidence', 'study']\n",
    "        support_score = sum(1 for word in support_keywords if word in premise_lower) * 0.2\n",
    "        \n",
    "        if support_score > 0.4:\n",
    "            return {\n",
    "                'ENTAILMENT': 0.7,\n",
    "                'NEUTRAL': 0.2,\n",
    "                'CONTRADICTION': 0.1\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'NEUTRAL': 0.6,\n",
    "                'ENTAILMENT': 0.2,\n",
    "                'CONTRADICTION': 0.2\n",
    "            }\n",
    "\n",
    "# Load NLI model and test\n",
    "nli_model = load_nli_model()\n",
    "\n",
    "# Test NLI inference\n",
    "test_premise = \"Clinical trials show side effects occur in less than 5% of patients and are mostly mild\"\n",
    "test_hypothesis = \"The new COVID vaccine causes severe side effects in 80% of patients\"\n",
    "\n",
    "nli_result = mock_nli_inference(test_premise, test_hypothesis)\n",
    "print(\"\\nNLI inference test:\")\n",
    "print(f\"Premise: {test_premise}\")\n",
    "print(f\"Hypothesis: {test_hypothesis}\")\n",
    "print(f\"Results: {nli_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8a7bb4",
   "metadata": {},
   "source": [
    "## Step 3: Evidence-Claim Stance Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_claim_evidence_stance(claim: str, evidence: Dict) -> Dict[str, any]:\n",
    "    \"\"\"Analyze stance between claim and evidence\"\"\"\n",
    "    # Get evidence text\n",
    "    evidence_text = evidence.get('content', '')\n",
    "    if evidence.get('key_snippets'):\n",
    "        evidence_text += ' ' + ' '.join(evidence['key_snippets'])\n",
    "    \n",
    "    # Perform NLI inference\n",
    "    nli_scores = mock_nli_inference(evidence_text, claim)\n",
    "    \n",
    "    # Determine stance\n",
    "    max_score = max(nli_scores.values())\n",
    "    stance = max(nli_scores.keys(), key=lambda k: nli_scores[k])\n",
    "    \n",
    "    # Map NLI labels to our verdict system\n",
    "    stance_mapping = {\n",
    "        'ENTAILMENT': Verdict.SUPPORTED,\n",
    "        'CONTRADICTION': Verdict.REFUTED,\n",
    "        'NEUTRAL': Verdict.UNCLEAR\n",
    "    }\n",
    "    \n",
    "    verdict = stance_mapping.get(stance, Verdict.UNCLEAR)\n",
    "    \n",
    "    # Calculate confidence based on score and source credibility\n",
    "    base_confidence = max_score\n",
    "    credibility_boost = evidence.get('credibility_score', 0.5) * 0.2\n",
    "    final_confidence = min(base_confidence + credibility_boost, 1.0)\n",
    "    \n",
    "    return {\n",
    "        'evidence_title': evidence.get('title', 'Unknown'),\n",
    "        'evidence_source': evidence.get('source_name', 'Unknown'),\n",
    "        'stance': verdict.value,\n",
    "        'confidence': final_confidence,\n",
    "        'nli_scores': nli_scores,\n",
    "        'credibility_score': evidence.get('credibility_score', 0.5),\n",
    "        'evidence_text': evidence_text[:200] + '...' if len(evidence_text) > 200 else evidence_text\n",
    "    }\n",
    "\n",
    "# Test stance detection\n",
    "print(\"Stance detection results:\")\n",
    "claim = test_claim_evidence['claim']\n",
    "print(f\"Claim: {claim}\\n\")\n",
    "\n",
    "stance_results = []\n",
    "for evidence in test_claim_evidence['evidence_sources']:\n",
    "    stance_result = analyze_claim_evidence_stance(claim, evidence)\n",
    "    stance_results.append(stance_result)\n",
    "    \n",
    "    print(f\"Evidence: {stance_result['evidence_title']}\")\n",
    "    print(f\"  Source: {stance_result['evidence_source']}\")\n",
    "    print(f\"  Stance: {stance_result['stance']}\")\n",
    "    print(f\"  Confidence: {stance_result['confidence']:.3f}\")\n",
    "    print(f\"  NLI Scores: {stance_result['nli_scores']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6048fb56",
   "metadata": {},
   "source": [
    "## Step 4: Aggregate Verdict Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ffc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_aggregate_verdict(stance_results: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"Calculate aggregate verdict from multiple evidence sources\"\"\"\n",
    "    if not stance_results:\n",
    "        return {\n",
    "            'verdict': Verdict.UNCLEAR.value,\n",
    "            'confidence': 0.0,\n",
    "            'reasoning': 'No evidence found'\n",
    "        }\n",
    "    \n",
    "    # Weight votes by confidence and credibility\n",
    "    weighted_votes = {\n",
    "        Verdict.SUPPORTED.value: 0,\n",
    "        Verdict.REFUTED.value: 0,\n",
    "        Verdict.UNCLEAR.value: 0\n",
    "    }\n",
    "    \n",
    "    total_weight = 0\n",
    "    evidence_count = {}\n",
    "    \n",
    "    for result in stance_results:\n",
    "        stance = result['stance']\n",
    "        confidence = result['confidence']\n",
    "        credibility = result['credibility_score']\n",
    "        \n",
    "        # Calculate weight (confidence * credibility)\n",
    "        weight = confidence * credibility\n",
    "        weighted_votes[stance] += weight\n",
    "        total_weight += weight\n",
    "        \n",
    "        # Count evidence by stance\n",
    "        evidence_count[stance] = evidence_count.get(stance, 0) + 1\n",
    "    \n",
    "    # Normalize weights\n",
    "    if total_weight > 0:\n",
    "        for stance in weighted_votes:\n",
    "            weighted_votes[stance] /= total_weight\n",
    "    \n",
    "    # Determine final verdict\n",
    "    final_verdict = max(weighted_votes.keys(), key=lambda k: weighted_votes[k])\n",
    "    final_confidence = weighted_votes[final_verdict]\n",
    "    \n",
    "    # Generate reasoning\n",
    "    reasoning_parts = []\n",
    "    for stance, count in evidence_count.items():\n",
    "        if count > 0:\n",
    "            reasoning_parts.append(f\"{count} source(s) {stance.lower()}\")\n",
    "    \n",
    "    reasoning = \"Based on \" + \", \".join(reasoning_parts) + \" the claim.\"\n",
    "    \n",
    "    return {\n",
    "        'verdict': final_verdict,\n",
    "        'confidence': final_confidence,\n",
    "        'weighted_votes': weighted_votes,\n",
    "        'evidence_count': evidence_count,\n",
    "        'reasoning': reasoning,\n",
    "        'total_sources': len(stance_results)\n",
    "    }\n",
    "\n",
    "def get_confidence_badge(verdict: str, confidence: float) -> Dict[str, str]:\n",
    "    \"\"\"Get confidence badge based on verdict and confidence score\"\"\"\n",
    "    if confidence >= 0.7:\n",
    "        if verdict == Verdict.SUPPORTED.value:\n",
    "            return {'emoji': '🟢', 'label': 'Likely True', 'color': 'green'}\n",
    "        elif verdict == Verdict.REFUTED.value:\n",
    "            return {'emoji': '🔴', 'label': 'Likely False', 'color': 'red'}\n",
    "        else:\n",
    "            return {'emoji': '🟡', 'label': 'Unclear', 'color': 'yellow'}\n",
    "    elif confidence >= 0.4:\n",
    "        return {'emoji': '🟡', 'label': 'Unclear', 'color': 'yellow'}\n",
    "    else:\n",
    "        return {'emoji': '⚫', 'label': 'Insufficient Evidence', 'color': 'gray'}\n",
    "\n",
    "# Test aggregate verdict calculation\n",
    "aggregate_result = calculate_aggregate_verdict(stance_results)\n",
    "confidence_badge = get_confidence_badge(aggregate_result['verdict'], aggregate_result['confidence'])\n",
    "\n",
    "print(\"Aggregate verdict calculation:\")\n",
    "print(f\"Final Verdict: {aggregate_result['verdict']}\")\n",
    "print(f\"Confidence: {aggregate_result['confidence']:.3f}\")\n",
    "print(f\"Badge: {confidence_badge['emoji']} {confidence_badge['label']}\")\n",
    "print(f\"Reasoning: {aggregate_result['reasoning']}\")\n",
    "print(f\"Weighted Votes: {aggregate_result['weighted_votes']}\")\n",
    "print(f\"Evidence Count: {aggregate_result['evidence_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60678075",
   "metadata": {},
   "source": [
    "## Step 5: Citation Extraction and Highlighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16be3add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(stance_results: List[Dict], max_citations: int = 3) -> List[Dict]:\n",
    "    \"\"\"Extract top citations with highlights\"\"\"\n",
    "    # Sort by confidence * credibility\n",
    "    sorted_results = sorted(stance_results, \n",
    "                          key=lambda x: x['confidence'] * x['credibility_score'], \n",
    "                          reverse=True)\n",
    "    \n",
    "    citations = []\n",
    "    for i, result in enumerate(sorted_results[:max_citations]):\n",
    "        citation = {\n",
    "            'id': f\"citation_{i+1}\",\n",
    "            'title': result['evidence_title'],\n",
    "            'source': result['evidence_source'],\n",
    "            'stance': result['stance'],\n",
    "            'confidence': result['confidence'],\n",
    "            'credibility': result['credibility_score'],\n",
    "            'text_snippet': result['evidence_text'],\n",
    "            'highlighted_text': highlight_relevant_text(result['evidence_text'], claim),\n",
    "            'weight': result['confidence'] * result['credibility_score']\n",
    "        }\n",
    "        citations.append(citation)\n",
    "    \n",
    "    return citations\n",
    "\n",
    "def highlight_relevant_text(text: str, claim: str) -> str:\n",
    "    \"\"\"Highlight relevant portions of text that relate to the claim\"\"\"\n",
    "    # Extract key terms from claim\n",
    "    claim_words = set(word.lower() for word in re.findall(r'\\w+', claim) \n",
    "                     if len(word) > 3)  # Filter short words\n",
    "    \n",
    "    # Highlight matching words in text\n",
    "    highlighted_text = text\n",
    "    for word in claim_words:\n",
    "        pattern = re.compile(re.escape(word), re.IGNORECASE)\n",
    "        highlighted_text = pattern.sub(f'**{word.upper()}**', highlighted_text)\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "def format_citation_summary(citations: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"Format citation summary for display\"\"\"\n",
    "    if not citations:\n",
    "        return {'count': 0, 'citations': []}\n",
    "    \n",
    "    formatted_citations = []\n",
    "    for citation in citations:\n",
    "        stance_icon = {\n",
    "            'SUPPORTED': '✅',\n",
    "            'REFUTED': '❌',\n",
    "            'UNCLEAR': '❓'\n",
    "        }.get(citation['stance'], '❓')\n",
    "        \n",
    "        formatted_citation = {\n",
    "            'id': citation['id'],\n",
    "            'display_title': f\"{stance_icon} {citation['title']}\",\n",
    "            'source': citation['source'],\n",
    "            'stance': citation['stance'],\n",
    "            'confidence_percent': f\"{citation['confidence']*100:.1f}%\",\n",
    "            'credibility_score': citation['credibility'],\n",
    "            'highlighted_snippet': citation['highlighted_text'][:150] + '...',\n",
    "            'weight_score': citation['weight']\n",
    "        }\n",
    "        formatted_citations.append(formatted_citation)\n",
    "    \n",
    "    return {\n",
    "        'count': len(citations),\n",
    "        'citations': formatted_citations,\n",
    "        'total_weight': sum(c['weight'] for c in citations)\n",
    "    }\n",
    "\n",
    "# Test citation extraction\n",
    "citations = extract_citations(stance_results)\n",
    "citation_summary = format_citation_summary(citations)\n",
    "\n",
    "print(\"Citation extraction results:\")\n",
    "print(f\"Total citations: {citation_summary['count']}\")\n",
    "print(f\"Total weight: {citation_summary['total_weight']:.3f}\\n\")\n",
    "\n",
    "for citation in citation_summary['citations']:\n",
    "    print(f\"Citation {citation['id']}:\")\n",
    "    print(f\"  Title: {citation['display_title']}\")\n",
    "    print(f\"  Source: {citation['source']}\")\n",
    "    print(f\"  Stance: {citation['stance']} (Confidence: {citation['confidence_percent']})\")\n",
    "    print(f\"  Highlighted: {citation['highlighted_snippet']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9738913c",
   "metadata": {},
   "source": [
    "## Step 6: Confidence Score Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86158611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_confidence_score(aggregate_result: Dict, citations: List[Dict]) -> Dict[str, any]:\n",
    "    \"\"\"Calibrate confidence score based on multiple factors\"\"\"\n",
    "    base_confidence = aggregate_result['confidence']\n",
    "    \n",
    "    # Factor 1: Number of sources\n",
    "    source_count = aggregate_result['total_sources']\n",
    "    source_bonus = min(source_count * 0.05, 0.15)  # Max 15% bonus\n",
    "    \n",
    "    # Factor 2: Source diversity (different types of sources)\n",
    "    source_types = set()\n",
    "    for citation in citations:\n",
    "        source_name = citation['source'].lower()\n",
    "        if 'government' in source_name or 'pib' in source_name or 'ministry' in source_name:\n",
    "            source_types.add('government')\n",
    "        elif 'who' in source_name or 'cdc' in source_name:\n",
    "            source_types.add('international')\n",
    "        elif 'wikipedia' in source_name:\n",
    "            source_types.add('encyclopedia')\n",
    "        else:\n",
    "            source_types.add('media')\n",
    "    \n",
    "    diversity_bonus = len(source_types) * 0.03  # 3% per source type\n",
    "    \n",
    "    # Factor 3: Consensus strength\n",
    "    verdict_distribution = aggregate_result['weighted_votes']\n",
    "    max_vote = max(verdict_distribution.values())\n",
    "    second_max_vote = sorted(verdict_distribution.values())[-2] if len(verdict_distribution) > 1 else 0\n",
    "    consensus_strength = max_vote - second_max_vote\n",
    "    consensus_bonus = consensus_strength * 0.1\n",
    "    \n",
    "    # Factor 4: High credibility source penalty/bonus\n",
    "    avg_credibility = sum(c['credibility'] for c in citations) / len(citations) if citations else 0.5\n",
    "    credibility_adjustment = (avg_credibility - 0.5) * 0.2\n",
    "    \n",
    "    # Calculate final calibrated confidence\n",
    "    calibrated_confidence = base_confidence + source_bonus + diversity_bonus + consensus_bonus + credibility_adjustment\n",
    "    calibrated_confidence = max(0.0, min(1.0, calibrated_confidence))  # Clamp to [0, 1]\n",
    "    \n",
    "    calibration_factors = {\n",
    "        'base_confidence': base_confidence,\n",
    "        'source_count_bonus': source_bonus,\n",
    "        'diversity_bonus': diversity_bonus,\n",
    "        'consensus_bonus': consensus_bonus,\n",
    "        'credibility_adjustment': credibility_adjustment,\n",
    "        'final_confidence': calibrated_confidence\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'calibrated_confidence': calibrated_confidence,\n",
    "        'confidence_change': calibrated_confidence - base_confidence,\n",
    "        'calibration_factors': calibration_factors,\n",
    "        'source_diversity': list(source_types),\n",
    "        'consensus_strength': consensus_strength\n",
    "    }\n",
    "\n",
    "def get_final_verdict_with_confidence(aggregate_result: Dict, calibration_result: Dict) -> Dict[str, any]:\n",
    "    \"\"\"Get final verdict with calibrated confidence\"\"\"\n",
    "    final_confidence = calibration_result['calibrated_confidence']\n",
    "    verdict = aggregate_result['verdict']\n",
    "    \n",
    "    # Get updated confidence badge\n",
    "    confidence_badge = get_confidence_badge(verdict, final_confidence)\n",
    "    \n",
    "    # Determine verdict certainty level\n",
    "    if final_confidence >= 0.8:\n",
    "        certainty = 'Very High'\n",
    "    elif final_confidence >= 0.7:\n",
    "        certainty = 'High'\n",
    "    elif final_confidence >= 0.5:\n",
    "        certainty = 'Medium'\n",
    "    elif final_confidence >= 0.3:\n",
    "        certainty = 'Low'\n",
    "    else:\n",
    "        certainty = 'Very Low'\n",
    "    \n",
    "    return {\n",
    "        'verdict': verdict,\n",
    "        'confidence': final_confidence,\n",
    "        'confidence_badge': confidence_badge,\n",
    "        'certainty_level': certainty,\n",
    "        'verdict_summary': f\"{confidence_badge['emoji']} {verdict} ({certainty} Confidence)\"\n",
    "    }\n",
    "\n",
    "# Test confidence calibration\n",
    "calibration_result = calibrate_confidence_score(aggregate_result, citations)\n",
    "final_verdict = get_final_verdict_with_confidence(aggregate_result, calibration_result)\n",
    "\n",
    "print(\"Confidence calibration results:\")\n",
    "print(f\"Original confidence: {aggregate_result['confidence']:.3f}\")\n",
    "print(f\"Calibrated confidence: {calibration_result['calibrated_confidence']:.3f}\")\n",
    "print(f\"Confidence change: {calibration_result['confidence_change']:+.3f}\")\n",
    "print(f\"\\nCalibration factors:\")\n",
    "for factor, value in calibration_result['calibration_factors'].items():\n",
    "    print(f\"  {factor}: {value:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal verdict: {final_verdict['verdict_summary']}\")\n",
    "print(f\"Certainty level: {final_verdict['certainty_level']}\")\n",
    "print(f\"Source diversity: {calibration_result['source_diversity']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8f69bb",
   "metadata": {},
   "source": [
    "## Step 7: Complete Phase 4 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ba1369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase4_pipeline(claim_evidence_data: Dict) -> Dict[str, any]:\n",
    "    \"\"\"Complete Phase 4 pipeline: Verification & Scoring\"\"\"\n",
    "    pipeline_result = {\n",
    "        'phase': 'Phase 4 - Verification & Scoring',\n",
    "        'input_claim': claim_evidence_data['claim'],\n",
    "        'steps': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        claim = claim_evidence_data['claim']\n",
    "        evidence_sources = claim_evidence_data['evidence_sources']\n",
    "        \n",
    "        # Step 1: Analyze stance for each evidence source\n",
    "        stance_results = []\n",
    "        for evidence in evidence_sources:\n",
    "            stance_result = analyze_claim_evidence_stance(claim, evidence)\n",
    "            stance_results.append(stance_result)\n",
    "        \n",
    "        pipeline_result['steps'].append({\n",
    "            'step': 'stance_analysis',\n",
    "            'result': {\n",
    "                'evidence_analyzed': len(stance_results),\n",
    "                'stance_distribution': {}\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Count stance distribution\n",
    "        stance_counts = {}\n",
    "        for result in stance_results:\n",
    "            stance = result['stance']\n",
    "            stance_counts[stance] = stance_counts.get(stance, 0) + 1\n",
    "        pipeline_result['steps'][0]['result']['stance_distribution'] = stance_counts\n",
    "        \n",
    "        # Step 2: Calculate aggregate verdict\n",
    "        aggregate_result = calculate_aggregate_verdict(stance_results)\n",
    "        pipeline_result['steps'].append({\n",
    "            'step': 'aggregate_verdict',\n",
    "            'result': aggregate_result\n",
    "        })\n",
    "        \n",
    "        # Step 3: Extract citations\n",
    "        citations = extract_citations(stance_results)\n",
    "        citation_summary = format_citation_summary(citations)\n",
    "        pipeline_result['steps'].append({\n",
    "            'step': 'citation_extraction',\n",
    "            'result': citation_summary\n",
    "        })\n",
    "        \n",
    "        # Step 4: Calibrate confidence\n",
    "        calibration_result = calibrate_confidence_score(aggregate_result, citations)\n",
    "        pipeline_result['steps'].append({\n",
    "            'step': 'confidence_calibration',\n",
    "            'result': calibration_result\n",
    "        })\n",
    "        \n",
    "        # Step 5: Generate final verdict\n",
    "        final_verdict = get_final_verdict_with_confidence(aggregate_result, calibration_result)\n",
    "        pipeline_result['steps'].append({\n",
    "            'step': 'final_verdict',\n",
    "            'result': final_verdict\n",
    "        })\n",
    "        \n",
    "        # Final output for Phase 5\n",
    "        pipeline_result['final_output'] = {\n",
    "            'claim': claim,\n",
    "            'verdict': final_verdict['verdict'],\n",
    "            'confidence': final_verdict['confidence'],\n",
    "            'confidence_badge': final_verdict['confidence_badge'],\n",
    "            'certainty_level': final_verdict['certainty_level'],\n",
    "            'citations': citation_summary['citations'],\n",
    "            'evidence_summary': {\n",
    "                'total_sources': len(evidence_sources),\n",
    "                'stance_distribution': stance_counts,\n",
    "                'avg_credibility': sum(r['credibility_score'] for r in stance_results) / len(stance_results) if stance_results else 0\n",
    "            },\n",
    "            'ready_for_phase5': True\n",
    "        }\n",
    "        pipeline_result['status'] = 'success'\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_result['error'] = str(e)\n",
    "        pipeline_result['status'] = 'failed'\n",
    "    \n",
    "    return pipeline_result\n",
    "\n",
    "# Test complete Phase 4 pipeline\n",
    "print(\"=== Testing Complete Phase 4 Pipeline ===\")\n",
    "phase4_result = phase4_pipeline(test_claim_evidence)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Status: {phase4_result['status']}\")\n",
    "final_output = phase4_result['final_output']\n",
    "print(f\"\\nClaim: {final_output['claim']}\")\n",
    "print(f\"Verdict: {final_output['verdict']}\")\n",
    "print(f\"Confidence: {final_output['confidence']:.3f}\")\n",
    "print(f\"Badge: {final_output['confidence_badge']['emoji']} {final_output['confidence_badge']['label']}\")\n",
    "print(f\"Certainty: {final_output['certainty_level']}\")\n",
    "\n",
    "print(f\"\\nEvidence Summary:\")\n",
    "print(f\"  Total sources: {final_output['evidence_summary']['total_sources']}\")\n",
    "print(f\"  Stance distribution: {final_output['evidence_summary']['stance_distribution']}\")\n",
    "print(f\"  Average credibility: {final_output['evidence_summary']['avg_credibility']:.3f}\")\n",
    "\n",
    "print(f\"\\nTop Citations:\")\n",
    "for citation in final_output['citations']:\n",
    "    print(f\"  {citation['display_title']}\")\n",
    "    print(f\"    Source: {citation['source']} (Confidence: {citation['confidence_percent']})\")\n",
    "    print(f\"    Snippet: {citation['highlighted_snippet']}\")\n",
    "\n",
    "print(f\"\\nReady for Phase 5: {final_output['ready_for_phase5']}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
