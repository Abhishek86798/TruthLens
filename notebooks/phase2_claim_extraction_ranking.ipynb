{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004b92c4",
   "metadata": {},
   "source": [
    "# Phase 2 — Claim Extraction & Ranking\n",
    "\n",
    "This notebook tests and debugs the claim extraction and ranking pipeline:\n",
    "- Extract atomic claims (simple seq2seq / regex rules)\n",
    "- Rank by check-worthiness → only factual-looking claims pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f56ac3",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b288e7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported all TruthLens modules\n",
      "\\n📊 Test Dataset Loaded:\n",
      "  • Total test texts: 10\n",
      "  • Text types: factual_claim, false_claim, opinion, conspiracy, mixed_content, statistical, conditional, attributed, hedged, direct_quote\n",
      "\\n🎯 Phase 2 Testing Focus:\n",
      "  • Claim detection (is_claim)\n",
      "  • Span extraction (extract_claim_spans)\n",
      "  • Atomicization (to_atomic)\n",
      "  • Context analysis (analyze_context)\n",
      "  • Claim ranking (score_claim)\n",
      "  • Complete pipeline (process_text)\n",
      "\\n✅ Ready to test TruthLens Phase 2 functionality!\n"
     ]
    }
   ],
   "source": [
    "# Phase 2: Claim Extraction and Ranking Testing Notebook\n",
    "# Purpose: Test claim detection, span extraction, atomicization, context analysis, and ranking\n",
    "# using actual TruthLens project modules\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import json\n",
    "\n",
    "# Ensure we can import from the project root\n",
    "project_root = Path(__file__).parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Try to import from current directory structure\n",
    "try:\n",
    "    # Import actual TruthLens claim extraction modules\n",
    "    from extractor.claim_detector import is_claim\n",
    "    from extractor.claim_extractor import extract_claim_spans\n",
    "    from extractor.atomicizer import to_atomic, AtomicClaim\n",
    "    from extractor.context import analyze_context\n",
    "    from extractor.ranker import score_claim\n",
    "    from extractor.pipeline import process_text\n",
    "    print(\"✅ Successfully imported all TruthLens modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")\n",
    "    print(\"📁 Current working directory:\", os.getcwd())\n",
    "    print(\"📁 Python path:\", sys.path[:3])\n",
    "    print(\"📁 Trying alternative import method...\")\n",
    "    \n",
    "    # Alternative: add both current and parent directories\n",
    "    current_dir = Path.cwd()\n",
    "    parent_dir = current_dir.parent\n",
    "    \n",
    "    if str(current_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(current_dir))\n",
    "    if str(parent_dir) not in sys.path:\n",
    "        sys.path.insert(0, str(parent_dir))\n",
    "    \n",
    "    # Try imports again\n",
    "    try:\n",
    "        from extractor.claim_detector import is_claim\n",
    "        from extractor.claim_extractor import extract_claim_spans\n",
    "        from extractor.atomicizer import to_atomic, AtomicClaim\n",
    "        from extractor.context import analyze_context\n",
    "        from extractor.ranker import score_claim\n",
    "        from extractor.pipeline import process_text\n",
    "        print(\"✅ Successfully imported TruthLens modules (alternative method)\")\n",
    "    except ImportError as e2:\n",
    "        print(f\"❌ Still failing: {e2}\")\n",
    "        # Final fallback - show available modules\n",
    "        print(\"📂 Available directories:\")\n",
    "        for item in Path.cwd().iterdir():\n",
    "            if item.is_dir():\n",
    "                print(f\"  {item.name}/\")\n",
    "\n",
    "# Test data for claim extraction and ranking\n",
    "test_texts = {\n",
    "    'factual_claim': \"COVID-19 vaccines reduce hospitalization rates by 90%.\",\n",
    "    'false_claim': \"5G towers cause COVID-19 transmission through radio waves.\",\n",
    "    'opinion': \"Pizza is the best food in the world.\",\n",
    "    'conspiracy': \"The government uses vaccines to implant tracking microchips.\",\n",
    "    'mixed_content': \"The WHO says vaccines are safe, but some Twitter users claim they cause autism.\",\n",
    "    'statistical': \"The unemployment rate decreased by 2.3% in Q3 2023.\",\n",
    "    'conditional': \"If global temperatures rise by 2 degrees, sea levels may increase significantly.\",\n",
    "    'attributed': \"According to Dr. Smith, vitamin D supplements boost immunity.\",\n",
    "    'hedged': \"Studies suggest that exercise might reduce depression symptoms.\",\n",
    "    'direct_quote': \"The president stated that 'inflation will be under control by next year.'\"\n",
    "}\n",
    "\n",
    "print(\"\\\\n📊 Test Dataset Loaded:\")\n",
    "print(f\"  • Total test texts: {len(test_texts)}\")\n",
    "print(f\"  • Text types: {', '.join(test_texts.keys())}\")\n",
    "print(\"\\\\n🎯 Phase 2 Testing Focus:\")\n",
    "print(\"  • Claim detection (is_claim)\")\n",
    "print(\"  • Span extraction (extract_claim_spans)\")\n",
    "print(\"  • Atomicization (to_atomic)\")\n",
    "print(\"  • Context analysis (analyze_context)\")\n",
    "print(\"  • Claim ranking (score_claim)\")\n",
    "print(\"  • Complete pipeline (process_text)\")\n",
    "print(\"\\\\n✅ Ready to test TruthLens Phase 2 functionality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c50bb8",
   "metadata": {},
   "source": [
    "## Step 2: Sentence-Level Claim Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ea983f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtest_claim_detection\u001b[39m(texts: \u001b[43mDict\u001b[49m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) -> Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28many\u001b[39m]]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Test claim detection using TruthLens is_claim function\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m \u001b[33;03m        Dictionary with detection results for each text\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     11\u001b[39m     results = {}\n",
      "\u001b[31mNameError\u001b[39m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def test_claim_detection(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test claim detection using TruthLens is_claim function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with detection results for each text\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 1: Claim Detection Using TruthLens ===\\n\")\n",
    "    \n",
    "    for label, text in texts.items():\n",
    "        try:\n",
    "            # Use TruthLens claim detector\n",
    "            is_claim_detected, confidence = is_claim(text)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'is_claim': is_claim_detected,\n",
    "                'confidence': confidence,\n",
    "                'threshold': 0.6,  # Default threshold used by is_claim\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_claim_detector'\n",
    "            }\n",
    "            \n",
    "            # Add interpretation\n",
    "            if is_claim_detected:\n",
    "                if confidence > 0.8:\n",
    "                    interpretation = \"Strong claim detected\"\n",
    "                elif confidence > 0.6:\n",
    "                    interpretation = \"Moderate claim detected\"\n",
    "                else:\n",
    "                    interpretation = \"Weak claim detected\"\n",
    "            else:\n",
    "                interpretation = \"No factual claim detected\"\n",
    "                \n",
    "            result['interpretation'] = interpretation\n",
    "            results[label] = result\n",
    "            \n",
    "            print(f\"📊 {label.upper()}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Is Claim: {is_claim_detected} (confidence: {confidence:.3f})\")\n",
    "            print(f\"Interpretation: {interpretation}\")\n",
    "            print(f\"Method: TruthLens ML-based claim detector\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run claim detection tests\n",
    "claim_detection_results = test_claim_detection(test_texts)\n",
    "\n",
    "# Summary statistics\n",
    "successful_tests = sum(1 for r in claim_detection_results.values() if r.get('status') == 'success')\n",
    "detected_claims = sum(1 for r in claim_detection_results.values() if r.get('is_claim', False))\n",
    "\n",
    "print(f\"\\n📈 CLAIM DETECTION SUMMARY:\")\n",
    "print(f\"✅ Successful tests: {successful_tests}/{len(test_texts)}\")\n",
    "print(f\"🎯 Claims detected: {detected_claims}/{len(test_texts)}\")\n",
    "print(f\"📊 Detection rate: {detected_claims/len(test_texts)*100:.1f}%\")\n",
    "\n",
    "# Expected vs Actual analysis\n",
    "expected_claims = ['factual_claim', 'conspiracy_claim', 'complex_claim', 'mixed_content']\n",
    "actual_claims = [label for label, result in claim_detection_results.items() if result.get('is_claim', False)]\n",
    "\n",
    "print(f\"\\n🔍 ANALYSIS:\")\n",
    "print(f\"Expected claims: {expected_claims}\")\n",
    "print(f\"Detected claims: {actual_claims}\")\n",
    "\n",
    "correctly_detected = set(expected_claims) & set(actual_claims)\n",
    "missed_claims = set(expected_claims) - set(actual_claims)\n",
    "false_positives = set(actual_claims) - set(expected_claims)\n",
    "\n",
    "if correctly_detected:\n",
    "    print(f\"✅ Correctly detected: {list(correctly_detected)}\")\n",
    "if missed_claims:\n",
    "    print(f\"❌ Missed claims: {list(missed_claims)}\")\n",
    "if false_positives:\n",
    "    print(f\"⚠️ False positives: {list(false_positives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9302c282",
   "metadata": {},
   "source": [
    "## Step 3: Atomic Claim Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b93f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_claim_span_extraction(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test \n",
    "    claim span extraction using TruthLens extract_claim_spans function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with span extraction results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 2: Claim Span Extraction Using TruthLens ===\\n\")\n",
    "    \n",
    "    for label, text in texts.items():\n",
    "        try:\n",
    "            # Use TruthLens claim span extractor\n",
    "            spans = extract_claim_spans(text)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'spans': spans,\n",
    "                'num_spans': len(spans),\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_span_extractor'\n",
    "            }\n",
    "            \n",
    "            # Process spans for display\n",
    "            processed_spans = []\n",
    "            for span in spans:\n",
    "                if hasattr(span, '__dict__'):\n",
    "                    # If span is an object with attributes\n",
    "                    span_data = {\n",
    "                        'text': getattr(span, 'text', ''),\n",
    "                        'start': getattr(span, 'start', 0),\n",
    "                        'end': getattr(span, 'end', 0),\n",
    "                        'confidence': getattr(span, 'conf', 0.0)\n",
    "                    }\n",
    "                elif isinstance(span, dict):\n",
    "                    # If span is a dictionary\n",
    "                    span_data = {\n",
    "                        'text': span.get('text', ''),\n",
    "                        'start': span.get('start', 0),\n",
    "                        'end': span.get('end', 0),\n",
    "                        'confidence': span.get('conf', 0.0)\n",
    "                    }\n",
    "                else:\n",
    "                    # Fallback for other types\n",
    "                    span_data = {'text': str(span), 'start': 0, 'end': len(str(span)), 'confidence': 0.5}\n",
    "                \n",
    "                processed_spans.append(span_data)\n",
    "            \n",
    "            result['processed_spans'] = processed_spans\n",
    "            results[label] = result\n",
    "            \n",
    "            print(f\"🎯 {label.upper()}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Number of spans found: {len(spans)}\")\n",
    "            \n",
    "            if processed_spans:\n",
    "                for i, span_data in enumerate(processed_spans, 1):\n",
    "                    print(f\"  Span {i}: '{span_data['text']}'\")\n",
    "                    print(f\"    Position: {span_data['start']}-{span_data['end']}\")\n",
    "                    print(f\"    Confidence: {span_data['confidence']:.3f}\")\n",
    "            else:\n",
    "                print(\"  No claim spans extracted\")\n",
    "                \n",
    "            print(f\"Method: TruthLens BIO/ML-based span extractor\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run claim span extraction tests\n",
    "span_extraction_results = test_claim_span_extraction(test_texts)\n",
    "\n",
    "# Summary statistics\n",
    "successful_extractions = sum(1 for r in span_extraction_results.values() if r.get('status') == 'success')\n",
    "total_spans = sum(r.get('num_spans', 0) for r in span_extraction_results.values() if r.get('status') == 'success')\n",
    "\n",
    "print(f\"\\n📈 SPAN EXTRACTION SUMMARY:\")\n",
    "print(f\"✅ Successful extractions: {successful_extractions}/{len(test_texts)}\")\n",
    "print(f\"🎯 Total spans extracted: {total_spans}\")\n",
    "print(f\"📊 Average spans per text: {total_spans/successful_extractions if successful_extractions > 0 else 0:.1f}\")\n",
    "\n",
    "# Show texts with most spans\n",
    "texts_with_spans = [(label, r.get('num_spans', 0)) for label, r in span_extraction_results.items() \n",
    "                   if r.get('status') == 'success' and r.get('num_spans', 0) > 0]\n",
    "texts_with_spans.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n🔍 TEXTS WITH EXTRACTED SPANS:\")\n",
    "for label, count in texts_with_spans:\n",
    "    spans_info = [f\"'{s['text']}'\" for s in span_extraction_results[label].get('processed_spans', [])]\n",
    "    print(f\"- {label}: {count} span(s) - {', '.join(spans_info)}\")\n",
    "\n",
    "if not texts_with_spans:\n",
    "    print(\"- No spans were extracted from any texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6f894a",
   "metadata": {},
   "source": [
    "## Step 4: Factual Content Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f43fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_atomic_decomposition(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test atomic claim decomposition using TruthLens to_atomic function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with atomic decomposition results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 3: Atomic Claim Decomposition Using TruthLens ===\\n\")\n",
    "    \n",
    "    # Add a specific test for complex claims that should decompose well\n",
    "    complex_test = \"Microsoft bought Activision Blizzard for $68.7 billion and Sony acquired Bungie for $3.6 billion in 2022.\"\n",
    "    test_set = {**texts, 'complex_acquisition': complex_test}\n",
    "    \n",
    "    for label, text in test_set.items():\n",
    "        try:\n",
    "            # Use TruthLens atomic decomposer\n",
    "            # to_atomic expects (span, srl_frames) - we'll pass None for srl_frames for simplicity\n",
    "            atomic_claims = to_atomic(text, None)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'atomic_claims': atomic_claims,\n",
    "                'num_atomic_claims': len(atomic_claims) if atomic_claims else 0,\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_atomicizer'\n",
    "            }\n",
    "            \n",
    "            # Process atomic claims for display\n",
    "            processed_claims = []\n",
    "            if atomic_claims:\n",
    "                for claim in atomic_claims:\n",
    "                    if isinstance(claim, dict):\n",
    "                        processed_claim = {\n",
    "                            'text': claim.get('text', ''),\n",
    "                            'subject': claim.get('subject', ''),\n",
    "                            'predicate': claim.get('predicate', ''),\n",
    "                            'object': claim.get('object', '')\n",
    "                        }\n",
    "                    else:\n",
    "                        # Fallback if claim is not a dict\n",
    "                        processed_claim = {\n",
    "                            'text': str(claim),\n",
    "                            'subject': '',\n",
    "                            'predicate': '',\n",
    "                            'object': ''\n",
    "                        }\n",
    "                    processed_claims.append(processed_claim)\n",
    "            \n",
    "            result['processed_claims'] = processed_claims\n",
    "            results[label] = result\n",
    "            \n",
    "            print(f\"⚛️ {label.upper()}\")\n",
    "            print(f\"Original: {text}\")\n",
    "            print(f\"Number of atomic claims: {len(atomic_claims) if atomic_claims else 0}\")\n",
    "            \n",
    "            if processed_claims:\n",
    "                for i, claim in enumerate(processed_claims, 1):\n",
    "                    print(f\"  Atomic Claim {i}:\")\n",
    "                    print(f\"    Text: '{claim['text']}'\")\n",
    "                    if claim['subject'] or claim['predicate'] or claim['object']:\n",
    "                        print(f\"    Subject: '{claim['subject']}'\")\n",
    "                        print(f\"    Predicate: '{claim['predicate']}'\")\n",
    "                        print(f\"    Object: '{claim['object']}'\")\n",
    "            else:\n",
    "                print(\"  No atomic decomposition available\")\n",
    "                \n",
    "            print(f\"Method: TruthLens rule-based + LLM atomicizer\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run atomic decomposition tests\n",
    "atomic_results = test_atomic_decomposition(test_texts)\n",
    "\n",
    "# Summary statistics\n",
    "successful_decompositions = sum(1 for r in atomic_results.values() if r.get('status') == 'success')\n",
    "total_atomic_claims = sum(r.get('num_atomic_claims', 0) for r in atomic_results.values() if r.get('status') == 'success')\n",
    "\n",
    "print(f\"\\n📈 ATOMIC DECOMPOSITION SUMMARY:\")\n",
    "print(f\"✅ Successful decompositions: {successful_decompositions}/{len(atomic_results)}\")\n",
    "print(f\"⚛️ Total atomic claims generated: {total_atomic_claims}\")\n",
    "print(f\"📊 Average atomic claims per text: {total_atomic_claims/successful_decompositions if successful_decompositions > 0 else 0:.1f}\")\n",
    "\n",
    "# Show texts that decomposed into multiple claims\n",
    "multi_claim_texts = [(label, r.get('num_atomic_claims', 0)) for label, r in atomic_results.items() \n",
    "                    if r.get('status') == 'success' and r.get('num_atomic_claims', 0) > 1]\n",
    "multi_claim_texts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n🔍 TEXTS WITH MULTIPLE ATOMIC CLAIMS:\")\n",
    "for label, count in multi_claim_texts:\n",
    "    print(f\"- {label}: {count} atomic claims\")\n",
    "    for i, claim in enumerate(atomic_results[label].get('processed_claims', []), 1):\n",
    "        print(f\"  {i}. {claim['text']}\")\n",
    "\n",
    "if not multi_claim_texts:\n",
    "    print(\"- No texts were decomposed into multiple atomic claims\")\n",
    "    \n",
    "print(f\"\\n🎯 Best decomposition examples:\")\n",
    "# Find the most successful decompositions\n",
    "best_examples = [(label, r) for label, r in atomic_results.items() \n",
    "                if r.get('status') == 'success' and r.get('num_atomic_claims', 0) > 0]\n",
    "\n",
    "for label, result in best_examples[:2]:  # Show top 2\n",
    "    claims = result.get('processed_claims', [])\n",
    "    if claims:\n",
    "        print(f\"- {label}: {result['text'][:50]}...\")\n",
    "        for claim in claims:\n",
    "            if claim['subject'] and claim['predicate'] and claim['object']:\n",
    "                print(f\"  → {claim['subject']} | {claim['predicate']} | {claim['object']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c743b03",
   "metadata": {},
   "source": [
    "## Step 5: Check-worthiness Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9913e9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context_analysis(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test context analysis using TruthLens analyze_context function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with context analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 4: Context Analysis Using TruthLens ===\\n\")\n",
    "    \n",
    "    # Create specific test cases for context analysis\n",
    "    context_test_cases = {\n",
    "        'definitive': \"The WHO confirms that COVID-19 vaccines are safe and effective.\",\n",
    "        'speculative': \"Some experts believe that vaccines might cause rare side effects.\",\n",
    "        'attributed': \"According to a Twitter post, vaccines are dangerous.\",\n",
    "        'hedged': \"It appears that social media may be spreading misinformation.\",\n",
    "        'uncertain': \"There could be unknown long-term effects of vaccines.\",\n",
    "        'direct': \"Vaccines reduce hospitalization by 90%.\"\n",
    "    }\n",
    "    \n",
    "    # Combine with original texts for comprehensive testing\n",
    "    all_test_cases = {**texts, **context_test_cases}\n",
    "    \n",
    "    for label, text in all_test_cases.items():\n",
    "        try:\n",
    "            # Use TruthLens context analyzer\n",
    "            # analyze_context expects (claim, sentence) - we'll use text for both\n",
    "            context_result = analyze_context(text, text)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'context_analysis': context_result,\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_context_analyzer'\n",
    "            }\n",
    "            \n",
    "            # Extract key context features\n",
    "            if isinstance(context_result, dict):\n",
    "                modality = context_result.get('modality', 'unknown')\n",
    "                attribution = context_result.get('attribution', 'none')\n",
    "                certainty = context_result.get('certainty', 'unknown')\n",
    "                hedging = context_result.get('hedging', False)\n",
    "                \n",
    "                result.update({\n",
    "                    'modality': modality,\n",
    "                    'attribution': attribution, \n",
    "                    'certainty': certainty,\n",
    "                    'hedging': hedging\n",
    "                })\n",
    "            else:\n",
    "                result.update({\n",
    "                    'modality': 'unknown',\n",
    "                    'attribution': 'none',\n",
    "                    'certainty': 'unknown',\n",
    "                    'hedging': False\n",
    "                })\n",
    "            \n",
    "            results[label] = result\n",
    "            \n",
    "            print(f\"🔍 {label.upper()}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            if isinstance(context_result, dict):\n",
    "                print(f\"Context Analysis:\")\n",
    "                for key, value in context_result.items():\n",
    "                    print(f\"  {key}: {value}\")\n",
    "            else:\n",
    "                print(f\"Context result: {context_result}\")\n",
    "            print(f\"Method: TruthLens linguistic context analyzer\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run context analysis tests\n",
    "context_results = test_context_analysis(test_texts)\n",
    "\n",
    "# Summary statistics\n",
    "successful_analyses = sum(1 for r in context_results.values() if r.get('status') == 'success')\n",
    "\n",
    "print(f\"\\n📈 CONTEXT ANALYSIS SUMMARY:\")\n",
    "print(f\"✅ Successful analyses: {successful_analyses}/{len(context_results)}\")\n",
    "\n",
    "# Group by modality\n",
    "modality_groups = {}\n",
    "attribution_groups = {}\n",
    "\n",
    "for label, result in context_results.items():\n",
    "    if result.get('status') == 'success':\n",
    "        modality = result.get('modality', 'unknown')\n",
    "        attribution = result.get('attribution', 'none')\n",
    "        \n",
    "        if modality not in modality_groups:\n",
    "            modality_groups[modality] = []\n",
    "        modality_groups[modality].append(label)\n",
    "        \n",
    "        if attribution not in attribution_groups:\n",
    "            attribution_groups[attribution] = []\n",
    "        attribution_groups[attribution].append(label)\n",
    "\n",
    "print(f\"\\n🔍 MODALITY DISTRIBUTION:\")\n",
    "for modality, labels in modality_groups.items():\n",
    "    print(f\"- {modality}: {labels}\")\n",
    "\n",
    "print(f\"\\n🔍 ATTRIBUTION DISTRIBUTION:\")  \n",
    "for attribution, labels in attribution_groups.items():\n",
    "    print(f\"- {attribution}: {labels}\")\n",
    "\n",
    "# Show examples of interesting context features\n",
    "print(f\"\\n🎯 INTERESTING CONTEXT FEATURES:\")\n",
    "for label, result in context_results.items():\n",
    "    if result.get('status') == 'success':\n",
    "        features = []\n",
    "        if result.get('hedging'):\n",
    "            features.append('hedged')\n",
    "        if result.get('attribution') and result.get('attribution') != 'none':\n",
    "            features.append(f\"attributed to: {result.get('attribution')}\")\n",
    "        if result.get('modality') and result.get('modality') != 'definitive':\n",
    "            features.append(f\"modality: {result.get('modality')}\")\n",
    "            \n",
    "        if features:\n",
    "            print(f\"- {label}: {', '.join(features)}\")\n",
    "\n",
    "print(f\"\\n✅ Context analysis helps determine:\")\n",
    "print(f\"  • How certain/definitive claims are\")\n",
    "print(f\"  • What sources are attributed\")\n",
    "print(f\"  • Language hedging and uncertainty markers\")\n",
    "print(f\"  • Modality (definitive, speculative, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5d79f",
   "metadata": {},
   "source": [
    "## Step 6: Claim Ranking and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_claim_scoring(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test claim scoring/ranking using TruthLens score_claim function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with claim scoring results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 5: Claim Scoring/Ranking Using TruthLens ===\\n\")\n",
    "    \n",
    "    # Add specific high-priority claims for testing\n",
    "    priority_test_cases = {\n",
    "        'high_priority_medical': \"COVID-19 vaccines cause blood clots in 50% of recipients.\",\n",
    "        'high_priority_tech': \"5G towers emit radiation that causes cancer.\",\n",
    "        'high_priority_conspiracy': \"The government is using vaccines to implant microchips for tracking.\",\n",
    "        'low_priority_opinion': \"Pizza is the best food in the world.\",\n",
    "        'low_priority_greeting': \"Have a wonderful day!\",\n",
    "        'medium_priority_politics': \"The election results were manipulated by foreign interference.\"\n",
    "    }\n",
    "    \n",
    "    # Combine with original texts\n",
    "    all_test_cases = {**texts, **priority_test_cases}\n",
    "    \n",
    "    scored_claims = []\n",
    "    \n",
    "    for label, text in all_test_cases.items():\n",
    "        try:\n",
    "            # Use TruthLens claim scorer\n",
    "            score = score_claim(text)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'score': score,\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_claim_scorer'\n",
    "            }\n",
    "            \n",
    "            # Categorize priority based on score\n",
    "            if score >= 0.8:\n",
    "                priority = \"High Priority\"\n",
    "                urgency = \"🔴 Immediate fact-check needed\"\n",
    "            elif score >= 0.6:\n",
    "                priority = \"Medium Priority\" \n",
    "                urgency = \"🟡 Should be fact-checked\"\n",
    "            elif score >= 0.4:\n",
    "                priority = \"Low-Medium Priority\"\n",
    "                urgency = \"🟢 Consider for fact-checking\"\n",
    "            else:\n",
    "                priority = \"Low Priority\"\n",
    "                urgency = \"⚪ Likely not worth fact-checking\"\n",
    "            \n",
    "            result.update({\n",
    "                'priority': priority,\n",
    "                'urgency': urgency\n",
    "            })\n",
    "            \n",
    "            results[label] = result\n",
    "            scored_claims.append((label, score, text))\n",
    "            \n",
    "            print(f\"🎯 {label.upper()}\")\n",
    "            print(f\"Text: {text}\")\n",
    "            print(f\"Check-worthiness Score: {score:.3f}\")\n",
    "            print(f\"Priority: {priority}\")\n",
    "            print(f\"Urgency: {urgency}\")\n",
    "            print(f\"Method: TruthLens ML/heuristic claim scorer\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results, scored_claims\n",
    "\n",
    "# Run claim scoring tests\n",
    "scoring_results, scored_claims = test_claim_scoring(test_texts)\n",
    "\n",
    "# Sort claims by score for ranking\n",
    "scored_claims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Summary statistics\n",
    "successful_scorings = sum(1 for r in scoring_results.values() if r.get('status') == 'success')\n",
    "avg_score = sum(r.get('score', 0) for r in scoring_results.values() if r.get('status') == 'success') / successful_scorings if successful_scorings > 0 else 0\n",
    "\n",
    "print(f\"\\n📈 CLAIM SCORING SUMMARY:\")\n",
    "print(f\"✅ Successful scorings: {successful_scorings}/{len(scoring_results)}\")\n",
    "print(f\"📊 Average score: {avg_score:.3f}\")\n",
    "\n",
    "# Priority distribution\n",
    "priority_counts = {}\n",
    "for result in scoring_results.values():\n",
    "    if result.get('status') == 'success':\n",
    "        priority = result.get('priority', 'Unknown')\n",
    "        priority_counts[priority] = priority_counts.get(priority, 0) + 1\n",
    "\n",
    "print(f\"\\n🎯 PRIORITY DISTRIBUTION:\")\n",
    "for priority, count in priority_counts.items():\n",
    "    print(f\"- {priority}: {count} claims\")\n",
    "\n",
    "# Top ranked claims\n",
    "print(f\"\\n🏆 TOP RANKED CLAIMS (by check-worthiness):\")\n",
    "for i, (label, score, text) in enumerate(scored_claims[:5], 1):\n",
    "    priority = scoring_results[label].get('priority', 'Unknown')\n",
    "    print(f\"{i}. {label} (Score: {score:.3f}, {priority})\")\n",
    "    print(f\"   Text: {text[:80]}...\")\n",
    "\n",
    "# Bottom ranked claims  \n",
    "print(f\"\\n📉 LOWEST RANKED CLAIMS:\")\n",
    "for i, (label, score, text) in enumerate(scored_claims[-3:], 1):\n",
    "    priority = scoring_results[label].get('priority', 'Unknown')\n",
    "    print(f\"{i}. {label} (Score: {score:.3f}, {priority})\")\n",
    "    print(f\"   Text: {text[:80]}...\")\n",
    "\n",
    "# Score distribution analysis\n",
    "high_scores = [s for _, s, _ in scored_claims if s >= 0.8]\n",
    "medium_scores = [s for _, s, _ in scored_claims if 0.4 <= s < 0.8]\n",
    "low_scores = [s for _, s, _ in scored_claims if s < 0.4]\n",
    "\n",
    "print(f\"\\n📊 SCORE DISTRIBUTION ANALYSIS:\")\n",
    "print(f\"🔴 High priority (≥0.8): {len(high_scores)} claims\")\n",
    "print(f\"🟡 Medium priority (0.4-0.8): {len(medium_scores)} claims\") \n",
    "print(f\"⚪ Low priority (<0.4): {len(low_scores)} claims\")\n",
    "\n",
    "if high_scores:\n",
    "    print(f\"🔴 High priority range: {min(high_scores):.3f} - {max(high_scores):.3f}\")\n",
    "if medium_scores:\n",
    "    print(f\"🟡 Medium priority range: {min(medium_scores):.3f} - {max(medium_scores):.3f}\")\n",
    "if low_scores:\n",
    "    print(f\"⚪ Low priority range: {min(low_scores):.3f} - {max(low_scores):.3f}\")\n",
    "\n",
    "print(f\"\\n✅ Claim scoring identifies which claims need immediate fact-checking attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d817980a",
   "metadata": {},
   "source": [
    "## Step 7: Complete Phase 2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec10c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_complete_claim_pipeline(texts: Dict[str, str]) -> Dict[str, Dict[str, any]]:\n",
    "    \"\"\"\n",
    "    Test complete claim extraction pipeline using TruthLens process_text function\n",
    "    \n",
    "    Args:\n",
    "        texts: Dictionary of test texts\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with complete pipeline results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"=== Step 6: Complete Claim Processing Pipeline ===\\n\")\n",
    "    \n",
    "    # Test with a comprehensive document\n",
    "    comprehensive_test = \"\"\"\n",
    "    Breaking News: Recent studies from Johns Hopkins University show that COVID-19 vaccines \n",
    "    reduce hospitalization rates by 90% among vaccinated individuals. However, social media \n",
    "    posts claim that vaccines cause severe side effects in 80% of recipients. According to \n",
    "    the WHO, these claims are unsubstantiated. Meanwhile, some people believe that 5G towers \n",
    "    are somehow connected to the spread of COVID-19, though scientists have debunked this theory.\n",
    "    The pharmaceutical company Pfizer reported $36.8 billion in revenue from their COVID-19 \n",
    "    vaccine in 2021. Critics argue that profit motives may influence vaccine recommendations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine with original texts plus comprehensive test\n",
    "    all_test_cases = {**texts, 'comprehensive_document': comprehensive_test}\n",
    "    \n",
    "    for label, text in all_test_cases.items():\n",
    "        try:\n",
    "            # Use TruthLens complete processing pipeline\n",
    "            pipeline_result = process_text(text)\n",
    "            \n",
    "            result = {\n",
    "                'text': text,\n",
    "                'pipeline_result': pipeline_result,\n",
    "                'num_claims_extracted': len(pipeline_result) if pipeline_result else 0,\n",
    "                'status': 'success',\n",
    "                'method': 'truthlens_complete_pipeline'\n",
    "            }\n",
    "            \n",
    "            # Process results for display\n",
    "            processed_claims = []\n",
    "            if pipeline_result:\n",
    "                for claim_data in pipeline_result:\n",
    "                    if isinstance(claim_data, dict):\n",
    "                        processed_claim = {\n",
    "                            'id': claim_data.get('id', ''),\n",
    "                            'text': claim_data.get('text', ''),\n",
    "                            'subject': claim_data.get('subject', ''),\n",
    "                            'predicate': claim_data.get('predicate', ''),\n",
    "                            'object': claim_data.get('object', ''),\n",
    "                            'checkworthiness': claim_data.get('checkworthiness', 0.0),\n",
    "                            'context': claim_data.get('context', {})\n",
    "                        }\n",
    "                        processed_claims.append(processed_claim)\n",
    "            \n",
    "            result['processed_claims'] = processed_claims\n",
    "            results[label] = result\n",
    "            \n",
    "            print(f\"🔄 {label.upper()}\")\n",
    "            print(f\"Input: {text[:100]}...\")\n",
    "            print(f\"Claims extracted: {len(pipeline_result) if pipeline_result else 0}\")\n",
    "            \n",
    "            if processed_claims:\n",
    "                # Sort by checkworthiness for display\n",
    "                sorted_claims = sorted(processed_claims, key=lambda x: x['checkworthiness'], reverse=True)\n",
    "                \n",
    "                for i, claim in enumerate(sorted_claims, 1):\n",
    "                    print(f\"\\n  Claim {i} (ID: {claim['id'][:8]}...):\")\n",
    "                    print(f\"    Text: '{claim['text']}'\")\n",
    "                    print(f\"    Checkworthiness: {claim['checkworthiness']:.3f}\")\n",
    "                    \n",
    "                    if claim['subject'] or claim['predicate'] or claim['object']:\n",
    "                        print(f\"    Structure: {claim['subject']} | {claim['predicate']} | {claim['object']}\")\n",
    "                    \n",
    "                    if claim['context']:\n",
    "                        context = claim['context']\n",
    "                        if isinstance(context, dict):\n",
    "                            context_features = []\n",
    "                            if context.get('modality'):\n",
    "                                context_features.append(f\"modality: {context['modality']}\")\n",
    "                            if context.get('attribution'):\n",
    "                                context_features.append(f\"attribution: {context['attribution']}\")\n",
    "                            if context_features:\n",
    "                                print(f\"    Context: {', '.join(context_features)}\")\n",
    "            else:\n",
    "                print(\"  No claims extracted by pipeline\")\n",
    "                \n",
    "            print(f\"Method: TruthLens end-to-end pipeline (detection→extraction→atomicization→context→scoring)\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[label] = {\n",
    "                'text': text,\n",
    "                'error': str(e),\n",
    "                'status': 'failed'\n",
    "            }\n",
    "            print(f\"❌ Error processing {label}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run complete pipeline tests\n",
    "pipeline_results = test_complete_claim_pipeline(test_texts)\n",
    "\n",
    "# Summary statistics\n",
    "successful_pipelines = sum(1 for r in pipeline_results.values() if r.get('status') == 'success')\n",
    "total_pipeline_claims = sum(r.get('num_claims_extracted', 0) for r in pipeline_results.values() if r.get('status') == 'success')\n",
    "\n",
    "print(f\"\\n📈 COMPLETE PIPELINE SUMMARY:\")\n",
    "print(f\"✅ Successful pipeline runs: {successful_pipelines}/{len(pipeline_results)}\")\n",
    "print(f\"🎯 Total claims extracted: {total_pipeline_claims}\")\n",
    "print(f\"📊 Average claims per document: {total_pipeline_claims/successful_pipelines if successful_pipelines > 0 else 0:.1f}\")\n",
    "\n",
    "# Find most productive documents\n",
    "productive_docs = [(label, r.get('num_claims_extracted', 0)) for label, r in pipeline_results.items() \n",
    "                  if r.get('status') == 'success' and r.get('num_claims_extracted', 0) > 0]\n",
    "productive_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n🏆 MOST PRODUCTIVE DOCUMENTS:\")\n",
    "for label, count in productive_docs[:3]:\n",
    "    print(f\"- {label}: {count} claims extracted\")\n",
    "\n",
    "# Show highest scoring claims across all documents\n",
    "all_claims = []\n",
    "for result in pipeline_results.values():\n",
    "    if result.get('status') == 'success':\n",
    "        for claim in result.get('processed_claims', []):\n",
    "            all_claims.append((claim['text'], claim['checkworthiness']))\n",
    "\n",
    "all_claims.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n🎯 HIGHEST PRIORITY CLAIMS (across all documents):\")\n",
    "for i, (claim_text, score) in enumerate(all_claims[:5], 1):\n",
    "    print(f\"{i}. Score {score:.3f}: {claim_text}\")\n",
    "\n",
    "print(f\"\\n=== Phase 2 Testing Complete ===\")\n",
    "print(\"✅ Claim detection, span extraction, atomicization, context analysis, and ranking all tested!\")\n",
    "print(\"✅ Complete end-to-end pipeline validated with actual TruthLens modules!\")\n",
    "print(\"✅ Ready to proceed to Phase 3: Evidence Retrieval\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
